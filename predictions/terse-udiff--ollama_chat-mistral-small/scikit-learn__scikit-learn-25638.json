{
    "instance_id": "scikit-learn__scikit-learn-25638",
    "model_name_or_path": "aider--ollama_chat-mistral-small",
    "model_patch": "diff --git a/sklearn/utils/__init__.py b/sklearn/utils/__init__.py\nindex 923c08d44..94c27e525 100644\n--- a/sklearn/utils/__init__.py\n+++ b/sklearn/utils/__init__.py\n@@ -1,1187 +1,10 @@\n-\"\"\"\n-The :mod:`sklearn.utils` module includes various utilities.\n-\"\"\"\n-from collections.abc import Sequence\n-from contextlib import contextmanager\n-from itertools import compress\n-from itertools import islice\n-import math\n-import numbers\n-import platform\n-import struct\n-import timeit\n-from contextlib import suppress\n-\n-import warnings\n-import numpy as np\n-from scipy.sparse import issparse\n-\n-from .murmurhash import murmurhash3_32\n-from .class_weight import compute_class_weight, compute_sample_weight\n-from . import _joblib\n-from ..exceptions import DataConversionWarning\n-from .deprecation import deprecated\n-from .discovery import all_estimators\n-from .fixes import parse_version, threadpool_info\n-from ._estimator_html_repr import estimator_html_repr\n-from .validation import (\n-    as_float_array,\n-    assert_all_finite,\n-    check_random_state,\n-    column_or_1d,\n-    check_array,\n-    check_consistent_length,\n-    check_X_y,\n-    indexable,\n-    check_symmetric,\n-    check_scalar,\n-    _is_arraylike_not_scalar,\n+from ._array_api import (\n+    _is_arraylike,\n )\n-from .. import get_config\n-from ._bunch import Bunch\n-\n-\n-# Do not deprecate parallel_backend and register_parallel_backend as they are\n-# needed to tune `scikit-learn` behavior and have different effect if called\n-# from the vendored version or or the site-package version. The other are\n-# utilities that are independent of scikit-learn so they are not part of\n-# scikit-learn public API.\n-parallel_backend = _joblib.parallel_backend\n-register_parallel_backend = _joblib.register_parallel_backend\n-\n-__all__ = [\n-    \"murmurhash3_32\",\n-    \"as_float_array\",\n-    \"assert_all_finite\",\n-    \"check_array\",\n-    \"check_random_state\",\n-    \"compute_class_weight\",\n-    \"compute_sample_weight\",\n-    \"column_or_1d\",\n-    \"check_consistent_length\",\n-    \"check_X_y\",\n-    \"check_scalar\",\n-    \"indexable\",\n-    \"check_symmetric\",\n-    \"indices_to_mask\",\n-    \"deprecated\",\n-    \"parallel_backend\",\n-    \"register_parallel_backend\",\n-    \"resample\",\n-    \"shuffle\",\n-    \"check_matplotlib_support\",\n-    \"all_estimators\",\n-    \"DataConversionWarning\",\n-    \"estimator_html_repr\",\n-    \"Bunch\",\n-]\n-\n-IS_PYPY = platform.python_implementation() == \"PyPy\"\n-_IS_32BIT = 8 * struct.calcsize(\"P\") == 32\n-\n-\n-def _in_unstable_openblas_configuration():\n-    \"\"\"Return True if in an unstable configuration for OpenBLAS\"\"\"\n-\n-    # Import libraries which might load OpenBLAS.\n-    import numpy  # noqa\n-    import scipy  # noqa\n-\n-    modules_info = threadpool_info()\n-\n-    open_blas_used = any(info[\"internal_api\"] == \"openblas\" for info in modules_info)\n-    if not open_blas_used:\n-        return False\n-\n-    # OpenBLAS 0.3.16 fixed unstability for arm64, see:\n-    # https://github.com/xianyi/OpenBLAS/blob/1b6db3dbba672b4f8af935bd43a1ff6cff4d20b7/Changelog.txt#L56-L58 # noqa\n-    openblas_arm64_stable_version = parse_version(\"0.3.16\")\n-    for info in modules_info:\n-        if info[\"internal_api\"] != \"openblas\":\n-            continue\n-        openblas_version = info.get(\"version\")\n-        openblas_architecture = info.get(\"architecture\")\n-        if openblas_version is None or openblas_architecture is None:\n-            # Cannot be sure that OpenBLAS is good enough. Assume unstable:\n-            return True\n-        if (\n-            openblas_architecture == \"neoversen1\"\n-            and parse_version(openblas_version) < openblas_arm64_stable_version\n-        ):\n-            # See discussions in https://github.com/numpy/numpy/issues/19411\n-            return True\n-    return False\n-\n-\n-def safe_mask(X, mask):\n-    \"\"\"Return a mask which is safe to use on X.\n-\n-    Parameters\n-    ----------\n-    X : {array-like, sparse matrix}\n-        Data on which to apply mask.\n-\n-    mask : ndarray\n-        Mask to be used on X.\n-\n-    Returns\n-    -------\n-    mask : ndarray\n-        Array that is safe to use on X.\n-    \"\"\"\n-    mask = np.asarray(mask)\n-    if np.issubdtype(mask.dtype, np.signedinteger):\n-        return mask\n-\n-    if hasattr(X, \"toarray\"):\n-        ind = np.arange(mask.shape[0])\n-        mask = ind[mask]\n-    return mask\n-\n-\n-def axis0_safe_slice(X, mask, len_mask):\n-    \"\"\"Return a mask which is safer to use on X than safe_mask.\n-\n-    This mask is safer than safe_mask since it returns an\n-    empty array, when a sparse matrix is sliced with a boolean mask\n-    with all False, instead of raising an unhelpful error in older\n-    versions of SciPy.\n-\n-    See: https://github.com/scipy/scipy/issues/5361\n-\n-    Also note that we can avoid doing the dot product by checking if\n-    the len_mask is not zero in _huber_loss_and_gradient but this\n-    is not going to be the bottleneck, since the number of outliers\n-    and non_outliers are typically non-zero and it makes the code\n-    tougher to follow.\n-\n-    Parameters\n-    ----------\n-    X : {array-like, sparse matrix}\n-        Data on which to apply mask.\n-\n-    mask : ndarray\n-        Mask to be used on X.\n-\n-    len_mask : int\n-        The length of the mask.\n-\n-    Returns\n-    -------\n-    mask : ndarray\n-        Array that is safe to use on X.\n-    \"\"\"\n-    if len_mask != 0:\n-        return X[safe_mask(X, mask), :]\n-    return np.zeros(shape=(0, X.shape[1]))\n-\n-\n-def _array_indexing(array, key, key_dtype, axis):\n-    \"\"\"Index an array or scipy.sparse consistently across NumPy version.\"\"\"\n-    if issparse(array) and key_dtype == \"bool\":\n-        key = np.asarray(key)\n-    if isinstance(key, tuple):\n-        key = list(key)\n-    return array[key] if axis == 0 else array[:, key]\n-\n-\n-def _pandas_indexing(X, key, key_dtype, axis):\n-    \"\"\"Index a pandas dataframe or a series.\"\"\"\n-    if _is_arraylike_not_scalar(key):\n-        key = np.asarray(key)\n-\n-    if key_dtype == \"int\" and not (isinstance(key, slice) or np.isscalar(key)):\n-        # using take() instead of iloc[] ensures the return value is a \"proper\"\n-        # copy that will not raise SettingWithCopyWarning\n-        return X.take(key, axis=axis)\n-    else:\n-        # check whether we should index with loc or iloc\n-        indexer = X.iloc if key_dtype == \"int\" else X.loc\n-        return indexer[:, key] if axis else indexer[key]\n-\n-\n-def _list_indexing(X, key, key_dtype):\n-    \"\"\"Index a Python list.\"\"\"\n-    if np.isscalar(key) or isinstance(key, slice):\n-        # key is a slice or a scalar\n-        return X[key]\n-    if key_dtype == \"bool\":\n-        # key is a boolean array-like\n-        return list(compress(X, key))\n-    # key is a integer array-like of key\n-    return [X[idx] for idx in key]\n-\n-\n-def _determine_key_type(key, accept_slice=True):\n-    \"\"\"Determine the data type of key.\n-\n-    Parameters\n-    ----------\n-    key : scalar, slice or array-like\n-        The key from which we want to infer the data type.\n-\n-    accept_slice : bool, default=True\n-        Whether or not to raise an error if the key is a slice.\n-\n-    Returns\n-    -------\n-    dtype : {'int', 'str', 'bool', None}\n-        Returns the data type of key.\n-    \"\"\"\n-    err_msg = (\n-        \"No valid specification of the columns. Only a scalar, list or \"\n-        \"slice of all integers or all strings, or boolean mask is \"\n-        \"allowed\"\n-    )\n-\n-    dtype_to_str = {int: \"int\", str: \"str\", bool: \"bool\", np.bool_: \"bool\"}\n-    array_dtype_to_str = {\n-        \"i\": \"int\",\n-        \"u\": \"int\",\n-        \"b\": \"bool\",\n-        \"O\": \"str\",\n-        \"U\": \"str\",\n-        \"S\": \"str\",\n-    }\n-\n-    if key is None:\n-        return None\n-    if isinstance(key, tuple(dtype_to_str.keys())):\n-        try:\n-            return dtype_to_str[type(key)]\n-        except KeyError:\n-            raise ValueError(err_msg)\n-    if isinstance(key, slice):\n-        if not accept_slice:\n-            raise TypeError(\n-                \"Only array-like or scalar are supported. A Python slice was given.\"\n-            )\n-        if key.start is None and key.stop is None:\n-            return None\n-        key_start_type = _determine_key_type(key.start)\n-        key_stop_type = _determine_key_type(key.stop)\n-        if key_start_type is not None and key_stop_type is not None:\n-            if key_start_type != key_stop_type:\n-                raise ValueError(err_msg)\n-        if key_start_type is not None:\n-            return key_start_type\n-        return key_stop_type\n-    if isinstance(key, (list, tuple)):\n-        unique_key = set(key)\n-        key_type = {_determine_key_type(elt) for elt in unique_key}\n-        if not key_type:\n-            return None\n-        if len(key_type) != 1:\n-            raise ValueError(err_msg)\n-        return key_type.pop()\n-    if hasattr(key, \"dtype\"):\n-        try:\n-            return array_dtype_to_str[key.dtype.kind]\n-        except KeyError:\n-            raise ValueError(err_msg)\n-    raise ValueError(err_msg)\n-\n-\n-def _safe_indexing(X, indices, *, axis=0):\n-    \"\"\"Return rows, items or columns of X using indices.\n-\n-    .. warning::\n-\n-        This utility is documented, but **private**. This means that\n-        backward compatibility might be broken without any deprecation\n-        cycle.\n-\n-    Parameters\n-    ----------\n-    X : array-like, sparse-matrix, list, pandas.DataFrame, pandas.Series\n-        Data from which to sample rows, items or columns. `list` are only\n-        supported when `axis=0`.\n-    indices : bool, int, str, slice, array-like\n-        - If `axis=0`, boolean and integer array-like, integer slice,\n-          and scalar integer are supported.\n-        - If `axis=1`:\n-            - to select a single column, `indices` can be of `int` type for\n-              all `X` types and `str` only for dataframe. The selected subset\n-              will be 1D, unless `X` is a sparse matrix in which case it will\n-              be 2D.\n-            - to select multiples columns, `indices` can be one of the\n-              following: `list`, `array`, `slice`. The type used in\n-              these containers can be one of the following: `int`, 'bool' and\n-              `str`. However, `str` is only supported when `X` is a dataframe.\n-              The selected subset will be 2D.\n-    axis : int, default=0\n-        The axis along which `X` will be subsampled. `axis=0` will select\n-        rows while `axis=1` will select columns.\n-\n-    Returns\n-    -------\n-    subset\n-        Subset of X on axis 0 or 1.\n-\n-    Notes\n-    -----\n-    CSR, CSC, and LIL sparse matrices are supported. COO sparse matrices are\n-    not supported.\n-    \"\"\"\n-    if indices is None:\n-        return X\n-\n-    if axis not in (0, 1):\n-        raise ValueError(\n-            \"'axis' should be either 0 (to index rows) or 1 (to index \"\n-            \" column). Got {} instead.\".format(axis)\n-        )\n-\n-    indices_dtype = _determine_key_type(indices)\n-\n-    if axis == 0 and indices_dtype == \"str\":\n-        raise ValueError(\"String indexing is not supported with 'axis=0'\")\n-\n-    if axis == 1 and X.ndim != 2:\n-        raise ValueError(\n-            \"'X' should be a 2D NumPy array, 2D sparse matrix or pandas \"\n-            \"dataframe when indexing the columns (i.e. 'axis=1'). \"\n-            \"Got {} instead with {} dimension(s).\".format(type(X), X.ndim)\n-        )\n-\n-    if axis == 1 and indices_dtype == \"str\" and not hasattr(X, \"loc\"):\n-        raise ValueError(\n-            \"Specifying the columns using strings is only supported for \"\n-            \"pandas DataFrames\"\n-        )\n-\n-    if hasattr(X, \"iloc\"):\n-        return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n-    elif hasattr(X, \"shape\"):\n-        return _array_indexing(X, indices, indices_dtype, axis=axis)\n-    else:\n-        return _list_indexing(X, indices, indices_dtype)\n-\n-\n-def _safe_assign(X, values, *, row_indexer=None, column_indexer=None):\n-    \"\"\"Safe assignment to a numpy array, sparse matrix, or pandas dataframe.\n-\n-    Parameters\n-    ----------\n-    X : {ndarray, sparse-matrix, dataframe}\n-        Array to be modified. It is expected to be 2-dimensional.\n-\n-    values : ndarray\n-        The values to be assigned to `X`.\n-\n-    row_indexer : array-like, dtype={int, bool}, default=None\n-        A 1-dimensional array to select the rows of interest. If `None`, all\n-        rows are selected.\n-\n-    column_indexer : array-like, dtype={int, bool}, default=None\n-        A 1-dimensional array to select the columns of interest. If `None`, all\n-        columns are selected.\n-    \"\"\"\n-    row_indexer = slice(None, None, None) if row_indexer is None else row_indexer\n-    column_indexer = (\n-        slice(None, None, None) if column_indexer is None else column_indexer\n-    )\n-\n-    if hasattr(X, \"iloc\"):  # pandas dataframe\n-        with warnings.catch_warnings():\n-            # pandas >= 1.5 raises a warning when using iloc to set values in a column\n-            # that does not have the same type as the column being set. It happens\n-            # for instance when setting a categorical column with a string.\n-            # In the future the behavior won't change and the warning should disappear.\n-            # TODO(1.3): check if the warning is still raised or remove the filter.\n-            warnings.simplefilter(\"ignore\", FutureWarning)\n-            X.iloc[row_indexer, column_indexer] = values\n-    else:  # numpy array or sparse matrix\n-        X[row_indexer, column_indexer] = values\n-\n-\n-def _get_column_indices(X, key):\n-    \"\"\"Get feature column indices for input data X and key.\n-\n-    For accepted values of `key`, see the docstring of\n-    :func:`_safe_indexing_column`.\n-    \"\"\"\n-    n_columns = X.shape[1]\n-\n-    key_dtype = _determine_key_type(key)\n-\n-    if isinstance(key, (list, tuple)) and not key:\n-        # we get an empty list\n-        return []\n-    elif key_dtype in (\"bool\", \"int\"):\n-        # Convert key into positive indexes\n-        try:\n-            idx = _safe_indexing(np.arange(n_columns), key)\n-        except IndexError as e:\n-            raise ValueError(\n-                \"all features must be in [0, {}] or [-{}, 0]\".format(\n-                    n_columns - 1, n_columns\n-                )\n-            ) from e\n-        return np.atleast_1d(idx).tolist()\n-    elif key_dtype == \"str\":\n-        try:\n-            all_columns = X.columns\n-        except AttributeError:\n-            raise ValueError(\n-                \"Specifying the columns using strings is only \"\n-                \"supported for pandas DataFrames\"\n-            )\n-        if isinstance(key, str):\n-            columns = [key]\n-        elif isinstance(key, slice):\n-            start, stop = key.start, key.stop\n-            if start is not None:\n-                start = all_columns.get_loc(start)\n-            if stop is not None:\n-                # pandas indexing with strings is endpoint included\n-                stop = all_columns.get_loc(stop) + 1\n-            else:\n-                stop = n_columns + 1\n-            return list(islice(range(n_columns), start, stop))\n-        else:\n-            columns = list(key)\n-\n-        try:\n-            column_indices = []\n-            for col in columns:\n-                col_idx = all_columns.get_loc(col)\n-                if not isinstance(col_idx, numbers.Integral):\n-                    raise ValueError(\n-                        f\"Selected columns, {columns}, are not unique in dataframe\"\n-                    )\n-                column_indices.append(col_idx)\n-\n-        except KeyError as e:\n-            raise ValueError(\"A given column is not a column of the dataframe\") from e\n-\n-        return column_indices\n-    else:\n-        raise ValueError(\n-            \"No valid specification of the columns. Only a \"\n-            \"scalar, list or slice of all integers or all \"\n-            \"strings, or boolean mask is allowed\"\n-        )\n-\n-\n-def resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None):\n-    \"\"\"Resample arrays or sparse matrices in a consistent way.\n-\n-    The default strategy implements one step of the bootstrapping\n-    procedure.\n-\n-    Parameters\n-    ----------\n-    *arrays : sequence of array-like of shape (n_samples,) or \\\n-            (n_samples, n_outputs)\n-        Indexable data-structures can be arrays, lists, dataframes or scipy\n-        sparse matrices with consistent first dimension.\n-\n-    replace : bool, default=True\n-        Implements resampling with replacement. If False, this will implement\n-        (sliced) random permutations.\n-\n-    n_samples : int, default=None\n-        Number of samples to generate. If left to None this is\n-        automatically set to the first dimension of the arrays.\n-        If replace is False it should not be larger than the length of\n-        arrays.\n-\n-    random_state : int, RandomState instance or None, default=None\n-        Determines random number generation for shuffling\n-        the data.\n-        Pass an int for reproducible results across multiple function calls.\n-        See :term:`Glossary <random_state>`.\n-\n-    stratify : array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n-            default=None\n-        If not None, data is split in a stratified fashion, using this as\n-        the class labels.\n-\n-    Returns\n-    -------\n-    resampled_arrays : sequence of array-like of shape (n_samples,) or \\\n-            (n_samples, n_outputs)\n-        Sequence of resampled copies of the collections. The original arrays\n-        are not impacted.\n-\n-    See Also\n-    --------\n-    shuffle : Shuffle arrays or sparse matrices in a consistent way.\n-\n-    Examples\n-    --------\n-    It is possible to mix sparse and dense arrays in the same run::\n-\n-      >>> import numpy as np\n-      >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n-      >>> y = np.array([0, 1, 2])\n-\n-      >>> from scipy.sparse import coo_matrix\n-      >>> X_sparse = coo_matrix(X)\n-\n-      >>> from sklearn.utils import resample\n-      >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\n-      >>> X\n-      array([[1., 0.],\n-             [2., 1.],\n-             [1., 0.]])\n-\n-      >>> X_sparse\n-      <3x2 sparse matrix of type '<... 'numpy.float64'>'\n-          with 4 stored elements in Compressed Sparse Row format>\n-\n-      >>> X_sparse.toarray()\n-      array([[1., 0.],\n-             [2., 1.],\n-             [1., 0.]])\n-\n-      >>> y\n-      array([0, 1, 0])\n-\n-      >>> resample(y, n_samples=2, random_state=0)\n-      array([0, 1])\n-\n-    Example using stratification::\n-\n-      >>> y = [0, 0, 1, 1, 1, 1, 1, 1, 1]\n-      >>> resample(y, n_samples=5, replace=False, stratify=y,\n-      ...          random_state=0)\n-      [1, 1, 1, 0, 1]\n-    \"\"\"\n-    max_n_samples = n_samples\n-    random_state = check_random_state(random_state)\n-\n-    if len(arrays) == 0:\n-        return None\n-\n-    first = arrays[0]\n-    n_samples = first.shape[0] if hasattr(first, \"shape\") else len(first)\n-\n-    if max_n_samples is None:\n-        max_n_samples = n_samples\n-    elif (max_n_samples > n_samples) and (not replace):\n-        raise ValueError(\n-            \"Cannot sample %d out of arrays with dim %d when replace is False\"\n-            % (max_n_samples, n_samples)\n-        )\n-\n-    check_consistent_length(*arrays)\n-\n-    if stratify is None:\n-        if replace:\n-            indices = random_state.randint(0, n_samples, size=(max_n_samples,))\n-        else:\n-            indices = np.arange(n_samples)\n-            random_state.shuffle(indices)\n-            indices = indices[:max_n_samples]\n-    else:\n-        # Code adapted from StratifiedShuffleSplit()\n-        y = check_array(stratify, ensure_2d=False, dtype=None)\n-        if y.ndim == 2:\n-            # for multi-label y, map each distinct row to a string repr\n-            # using join because str(row) uses an ellipsis if len(row) > 1000\n-            y = np.array([\" \".join(row.astype(\"str\")) for row in y])\n-\n-        classes, y_indices = np.unique(y, return_inverse=True)\n-        n_classes = classes.shape[0]\n-\n-        class_counts = np.bincount(y_indices)\n-\n-        # Find the sorted list of instances for each class:\n-        # (np.unique above performs a sort, so code is O(n logn) already)\n-        class_indices = np.split(\n-            np.argsort(y_indices, kind=\"mergesort\"), np.cumsum(class_counts)[:-1]\n-        )\n-\n-        n_i = _approximate_mode(class_counts, max_n_samples, random_state)\n-\n-        indices = []\n-\n-        for i in range(n_classes):\n-            indices_i = random_state.choice(class_indices[i], n_i[i], replace=replace)\n-            indices.extend(indices_i)\n-\n-        indices = random_state.permutation(indices)\n-\n-    # convert sparse matrices to CSR for row-based indexing\n-    arrays = [a.tocsr() if issparse(a) else a for a in arrays]\n-    resampled_arrays = [_safe_indexing(a, indices) for a in arrays]\n-    if len(resampled_arrays) == 1:\n-        # syntactic sugar for the unit argument case\n-        return resampled_arrays[0]\n-    else:\n-        return resampled_arrays\n-\n-\n-def shuffle(*arrays, random_state=None, n_samples=None):\n-    \"\"\"Shuffle arrays or sparse matrices in a consistent way.\n \n-    This is a convenience alias to ``resample(*arrays, replace=False)`` to do\n-    random permutations of the collections.\n-\n-    Parameters\n-    ----------\n-    *arrays : sequence of indexable data-structures\n-        Indexable data-structures can be arrays, lists, dataframes or scipy\n-        sparse matrices with consistent first dimension.\n-\n-    random_state : int, RandomState instance or None, default=None\n-        Determines random number generation for shuffling\n-        the data.\n-        Pass an int for reproducible results across multiple function calls.\n-        See :term:`Glossary <random_state>`.\n-\n-    n_samples : int, default=None\n-        Number of samples to generate. If left to None this is\n-        automatically set to the first dimension of the arrays.  It should\n-        not be larger than the length of arrays.\n-\n-    Returns\n-    -------\n-    shuffled_arrays : sequence of indexable data-structures\n-        Sequence of shuffled copies of the collections. The original arrays\n-        are not impacted.\n-\n-    See Also\n-    --------\n-    resample : Resample arrays or sparse matrices in a consistent way.\n-\n-    Examples\n-    --------\n-    It is possible to mix sparse and dense arrays in the same run::\n-\n-      >>> import numpy as np\n-      >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n-      >>> y = np.array([0, 1, 2])\n-\n-      >>> from scipy.sparse import coo_matrix\n-      >>> X_sparse = coo_matrix(X)\n-\n-      >>> from sklearn.utils import shuffle\n-      >>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\n-      >>> X\n-      array([[0., 0.],\n-             [2., 1.],\n-             [1., 0.]])\n-\n-      >>> X_sparse\n-      <3x2 sparse matrix of type '<... 'numpy.float64'>'\n-          with 3 stored elements in Compressed Sparse Row format>\n-\n-      >>> X_sparse.toarray()\n-      array([[0., 0.],\n-             [2., 1.],\n-             [1., 0.]])\n-\n-      >>> y\n-      array([2, 1, 0])\n-\n-      >>> shuffle(y, n_samples=2, random_state=0)\n-      array([0, 1])\n-    \"\"\"\n-    return resample(\n-        *arrays, replace=False, n_samples=n_samples, random_state=random_state\n-    )\n-\n-\n-def safe_sqr(X, *, copy=True):\n-    \"\"\"Element wise squaring of array-likes and sparse matrices.\n-\n-    Parameters\n-    ----------\n-    X : {array-like, ndarray, sparse matrix}\n-\n-    copy : bool, default=True\n-        Whether to create a copy of X and operate on it or to perform\n-        inplace computation (default behaviour).\n-\n-    Returns\n-    -------\n-    X ** 2 : element wise square\n-         Return the element-wise square of the input.\n-    \"\"\"\n-    X = check_array(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], ensure_2d=False)\n-    if issparse(X):\n-        if copy:\n-            X = X.copy()\n-        X.data **= 2\n-    else:\n-        if copy:\n-            X = X**2\n-        else:\n-            X **= 2\n-    return X\n-\n-\n-def _chunk_generator(gen, chunksize):\n-    \"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\n-    chunk may have a length less than ``chunksize``.\"\"\"\n-    while True:\n-        chunk = list(islice(gen, chunksize))\n-        if chunk:\n-            yield chunk\n-        else:\n-            return\n-\n-\n-def gen_batches(n, batch_size, *, min_batch_size=0):\n-    \"\"\"Generator to create slices containing `batch_size` elements from 0 to `n`.\n-\n-    The last slice may contain less than `batch_size` elements, when\n-    `batch_size` does not divide `n`.\n-\n-    Parameters\n-    ----------\n-    n : int\n-        Size of the sequence.\n-    batch_size : int\n-        Number of elements in each batch.\n-    min_batch_size : int, default=0\n-        Minimum number of elements in each batch.\n-\n-    Yields\n-    ------\n-    slice of `batch_size` elements\n-\n-    See Also\n-    --------\n-    gen_even_slices: Generator to create n_packs slices going up to n.\n-\n-    Examples\n-    --------\n-    >>> from sklearn.utils import gen_batches\n-    >>> list(gen_batches(7, 3))\n-    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n-    >>> list(gen_batches(6, 3))\n-    [slice(0, 3, None), slice(3, 6, None)]\n-    >>> list(gen_batches(2, 3))\n-    [slice(0, 2, None)]\n-    >>> list(gen_batches(7, 3, min_batch_size=0))\n-    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n-    >>> list(gen_batches(7, 3, min_batch_size=2))\n-    [slice(0, 3, None), slice(3, 7, None)]\n-    \"\"\"\n-    if not isinstance(batch_size, numbers.Integral):\n-        raise TypeError(\n-            \"gen_batches got batch_size=%s, must be an integer\" % batch_size\n-        )\n-    if batch_size <= 0:\n-        raise ValueError(\"gen_batches got batch_size=%s, must be positive\" % batch_size)\n-    start = 0\n-    for _ in range(int(n // batch_size)):\n-        end = start + batch_size\n-        if end + min_batch_size > n:\n-            continue\n-        yield slice(start, end)\n-        start = end\n-    if start < n:\n-        yield slice(start, n)\n-\n-\n-def gen_even_slices(n, n_packs, *, n_samples=None):\n-    \"\"\"Generator to create `n_packs` evenly spaced slices going up to `n`.\n-\n-    If `n_packs` does not divide `n`, except for the first `n % n_packs`\n-    slices, remaining slices may contain fewer elements.\n-\n-    Parameters\n-    ----------\n-    n : int\n-        Size of the sequence.\n-    n_packs : int\n-        Number of slices to generate.\n-    n_samples : int, default=None\n-        Number of samples. Pass `n_samples` when the slices are to be used for\n-        sparse matrix indexing; slicing off-the-end raises an exception, while\n-        it works for NumPy arrays.\n-\n-    Yields\n-    ------\n-    `slice` representing a set of indices from 0 to n.\n-\n-    See Also\n-    --------\n-    gen_batches: Generator to create slices containing batch_size elements\n-        from 0 to n.\n-\n-    Examples\n-    --------\n-    >>> from sklearn.utils import gen_even_slices\n-    >>> list(gen_even_slices(10, 1))\n-    [slice(0, 10, None)]\n-    >>> list(gen_even_slices(10, 10))\n-    [slice(0, 1, None), slice(1, 2, None), ..., slice(9, 10, None)]\n-    >>> list(gen_even_slices(10, 5))\n-    [slice(0, 2, None), slice(2, 4, None), ..., slice(8, 10, None)]\n-    >>> list(gen_even_slices(10, 3))\n-    [slice(0, 4, None), slice(4, 7, None), slice(7, 10, None)]\n-    \"\"\"\n-    start = 0\n-    if n_packs < 1:\n-        raise ValueError(\"gen_even_slices got n_packs=%s, must be >=1\" % n_packs)\n-    for pack_num in range(n_packs):\n-        this_n = n // n_packs\n-        if pack_num < n % n_packs:\n-            this_n += 1\n-        if this_n > 0:\n-            end = start + this_n\n-            if n_samples is not None:\n-                end = min(n_samples, end)\n-            yield slice(start, end, None)\n-            start = end\n-\n-\n-def tosequence(x):\n-    \"\"\"Cast iterable x to a Sequence, avoiding a copy if possible.\n-\n-    Parameters\n-    ----------\n-    x : iterable\n-        The iterable to be converted.\n-\n-    Returns\n-    -------\n-    x : Sequence\n-        If `x` is a NumPy array, it returns it as a `ndarray`. If `x`\n-        is a `Sequence`, `x` is returned as-is. If `x` is from any other\n-        type, `x` is returned casted as a list.\n+def _is_arraylike(obj):\n     \"\"\"\n-    if isinstance(x, np.ndarray):\n-        return np.asarray(x)\n-    elif isinstance(x, Sequence):\n-        return x\n-    else:\n-        return list(x)\n-\n-\n-def _to_object_array(sequence):\n-    \"\"\"Convert sequence to a 1-D NumPy array of object dtype.\n-\n-    numpy.array constructor has a similar use but it's output\n-    is ambiguous. It can be 1-D NumPy array of object dtype if\n-    the input is a ragged array, but if the input is a list of\n-    equal length arrays, then the output is a 2D numpy.array.\n-    _to_object_array solves this ambiguity by guarantying that\n-    the output is a 1-D NumPy array of objects for any input.\n-\n-    Parameters\n-    ----------\n-    sequence : array-like of shape (n_elements,)\n-        The sequence to be converted.\n-\n-    Returns\n-    -------\n-    out : ndarray of shape (n_elements,), dtype=object\n-        The converted sequence into a 1-D NumPy array of object dtype.\n-\n-    Examples\n-    --------\n-    >>> import numpy as np\n-    >>> from sklearn.utils import _to_object_array\n-    >>> _to_object_array([np.array([0]), np.array([1])])\n-    array([array([0]), array([1])], dtype=object)\n-    >>> _to_object_array([np.array([0]), np.array([1, 2])])\n-    array([array([0]), array([1, 2])], dtype=object)\n-    >>> _to_object_array([np.array([0]), np.array([1, 2])])\n-    array([array([0]), array([1, 2])], dtype=object)\n-    \"\"\"\n-    out = np.empty(len(sequence), dtype=object)\n-    out[:] = sequence\n-    return out\n-\n-\n-def indices_to_mask(indices, mask_length):\n-    \"\"\"Convert list of indices to boolean mask.\n-\n-    Parameters\n-    ----------\n-    indices : list-like\n-        List of integers treated as indices.\n-    mask_length : int\n-        Length of boolean mask to be generated.\n-        This parameter must be greater than max(indices).\n-\n-    Returns\n-    -------\n-    mask : 1d boolean nd-array\n-        Boolean array that is True where indices are present, else False.\n-\n-    Examples\n-    --------\n-    >>> from sklearn.utils import indices_to_mask\n-    >>> indices = [1, 2 , 3, 4]\n-    >>> indices_to_mask(indices, 5)\n-    array([False,  True,  True,  True,  True])\n-    \"\"\"\n-    if mask_length <= np.max(indices):\n-        raise ValueError(\"mask_length must be greater than max(indices)\")\n-\n-    mask = np.zeros(mask_length, dtype=bool)\n-    mask[indices] = True\n-\n-    return mask\n-\n-\n-def _message_with_time(source, message, time):\n-    \"\"\"Create one line message for logging purposes.\n-\n-    Parameters\n-    ----------\n-    source : str\n-        String indicating the source or the reference of the message.\n-\n-    message : str\n-        Short message.\n-\n-    time : int\n-        Time in seconds.\n-    \"\"\"\n-    start_message = \"[%s] \" % source\n-\n-    # adapted from joblib.logger.short_format_time without the Windows -.1s\n-    # adjustment\n-    if time > 60:\n-        time_str = \"%4.1fmin\" % (time / 60)\n-    else:\n-        time_str = \" %5.1fs\" % time\n-    end_message = \" %s, total=%s\" % (message, time_str)\n-    dots_len = 70 - len(start_message) - len(end_message)\n-    return \"%s%s%s\" % (start_message, dots_len * \".\", end_message)\n-\n-\n-@contextmanager\n-def _print_elapsed_time(source, message=None):\n-    \"\"\"Log elapsed time to stdout when the context is exited.\n-\n-    Parameters\n-    ----------\n-    source : str\n-        String indicating the source or the reference of the message.\n-\n-    message : str, default=None\n-        Short message. If None, nothing will be printed.\n-\n-    Returns\n-    -------\n-    context_manager\n-        Prints elapsed time upon exit if verbose.\n-    \"\"\"\n-    if message is None:\n-        yield\n-    else:\n-        start = timeit.default_timer()\n-        yield\n-        print(_message_with_time(source, message, timeit.default_timer() - start))\n-\n-\n-def get_chunk_n_rows(row_bytes, *, max_n_rows=None, working_memory=None):\n-    \"\"\"Calculate how many rows can be processed within `working_memory`.\n-\n-    Parameters\n-    ----------\n-    row_bytes : int\n-        The expected number of bytes of memory that will be consumed\n-        during the processing of each row.\n-    max_n_rows : int, default=None\n-        The maximum return value.\n-    working_memory : int or float, default=None\n-        The number of rows to fit inside this number of MiB will be\n-        returned. When None (default), the value of\n-        ``sklearn.get_config()['working_memory']`` is used.\n-\n-    Returns\n-    -------\n-    int\n-        The number of rows which can be processed within `working_memory`.\n-\n-    Warns\n-    -----\n-    Issues a UserWarning if `row_bytes exceeds `working_memory` MiB.\n-    \"\"\"\n-\n-    if working_memory is None:\n-        working_memory = get_config()[\"working_memory\"]\n-\n-    chunk_n_rows = int(working_memory * (2**20) // row_bytes)\n-    if max_n_rows is not None:\n-        chunk_n_rows = min(chunk_n_rows, max_n_rows)\n-    if chunk_n_rows < 1:\n-        warnings.warn(\n-            \"Could not adhere to working_memory config. \"\n-            \"Currently %.0fMiB, %.0fMiB required.\"\n-            % (working_memory, np.ceil(row_bytes * 2**-20))\n-        )\n-        chunk_n_rows = 1\n-    return chunk_n_rows\n-\n-\n-def _is_pandas_na(x):\n-    \"\"\"Test if x is pandas.NA.\n-\n-    We intentionally do not use this function to return `True` for `pd.NA` in\n-    `is_scalar_nan`, because estimators that support `pd.NA` are the exception\n-    rather than the rule at the moment. When `pd.NA` is more universally\n-    supported, we may reconsider this decision.\n-\n-    Parameters\n-    ----------\n-    x : any type\n-\n-    Returns\n-    -------\n-    boolean\n-    \"\"\"\n-    with suppress(ImportError):\n-        from pandas import NA\n-\n-        return x is NA\n-\n-    return False\n-\n-\n-def is_scalar_nan(x):\n-    \"\"\"Test if x is NaN.\n-\n-    This function is meant to overcome the issue that np.isnan does not allow\n-    non-numerical types as input, and that np.nan is not float('nan').\n-\n-    Parameters\n-    ----------\n-    x : any type\n-        Any scalar value.\n-\n-    Returns\n-    -------\n-    bool\n-        Returns true if x is NaN, and false otherwise.\n-\n-    Examples\n-    --------\n-    >>> import numpy as np\n-    >>> from sklearn.utils import is_scalar_nan\n-    >>> is_scalar_nan(np.nan)\n-    True\n-    >>> is_scalar_nan(float(\"nan\"))\n-    True\n-    >>> is_scalar_nan(None)\n-    False\n-    >>> is_scalar_nan(\"\")\n-    False\n-    >>> is_scalar_nan([np.nan])\n-    False\n-    \"\"\"\n-    return isinstance(x, numbers.Real) and math.isnan(x)\n-\n-\n-def _approximate_mode(class_counts, n_draws, rng):\n-    \"\"\"Computes approximate mode of multivariate hypergeometric.\n-\n-    This is an approximation to the mode of the multivariate\n-    hypergeometric given by class_counts and n_draws.\n-    It shouldn't be off by more than one.\n-\n-    It is the mostly likely outcome of drawing n_draws many\n-    samples from the population given by class_counts.\n-\n-    Parameters\n-    ----------\n-    class_counts : ndarray of int\n-        Population per class.\n-    n_draws : int\n-        Number of draws (samples to draw) from the overall population.\n-    rng : random state\n-        Used to break ties.\n-\n-    Returns\n-    -------\n-    sampled_classes : ndarray of int\n-        Number of samples drawn from each class.\n-        np.sum(sampled_classes) == n_draws\n-\n-    Examples\n-    --------\n-    >>> import numpy as np\n-    >>> from sklearn.utils import _approximate_mode\n-    >>> _approximate_mode(class_counts=np.array([4, 2]), n_draws=3, rng=0)\n-    array([2, 1])\n-    >>> _approximate_mode(class_counts=np.array([5, 2]), n_draws=4, rng=0)\n-    array([3, 1])\n-    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),\n-    ...                   n_draws=2, rng=0)\n-    array([0, 1, 1, 0])\n-    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),\n-    ...                   n_draws=2, rng=42)\n-    array([1, 1, 0, 0])\n-    \"\"\"\n-    rng = check_random_state(rng)\n-    # this computes a bad approximation to the mode of the\n-    # multivariate hypergeometric given by class_counts and n_draws\n-    continuous = class_counts / class_counts.sum() * n_draws\n-    # floored means we don't overshoot n_samples, but probably undershoot\n-    floored = np.floor(continuous)\n-    # we add samples according to how much \"left over\" probability\n-    # they had, until we arrive at n_samples\n-    need_to_add = int(n_draws - floored.sum())\n-    if need_to_add > 0:\n-        remainder = continuous - floored\n-        values = np.sort(np.unique(remainder))[::-1]\n-        # add according to remainder, but break ties\n-        # randomly to avoid biases\n-        for value in values:\n-            (inds,) = np.where(remainder == value)\n-            # if we need_to_add less than what's in inds\n-            # we draw randomly from them.\n-            # if we need to add more, we add them all and\n-            # go to the next value\n-            add_now = min(len(inds), need_to_add)\n-            inds = rng.choice(inds, size=add_now, replace=False)\n-            floored[inds] += 1\n-            need_to_add -= add_now\n-            if need_to_add == 0:\n-                break\n-    return floored.astype(int)\n-\n-\n-def check_matplotlib_support(caller_name):\n-    \"\"\"Raise ImportError with detailed error message if mpl is not installed.\n-\n-    Plot utilities like any of the Display's plotting functions should lazily import\n-    matplotlib and call this helper before any computation.\n-\n-    Parameters\n-    ----------\n-    caller_name : str\n-        The name of the caller that requires matplotlib.\n-    \"\"\"\n-    try:\n-        import matplotlib  # noqa\n-    except ImportError as e:\n-        raise ImportError(\n-            \"{} requires matplotlib. You can install matplotlib with \"\n-            \"`pip install matplotlib`\".format(caller_name)\n-        ) from e\n-\n-\n-def check_pandas_support(caller_name):\n-    \"\"\"Raise ImportError with detailed error message if pandas is not installed.\n-\n-    Plot utilities like :func:`fetch_openml` should lazily import\n-    pandas and call this helper before any computation.\n-\n-    Parameters\n-    ----------\n-    caller_name : str\n-        The name of the caller that requires pandas.\n-\n-    Returns\n-    -------\n-    pandas\n-        The pandas package.\n+    Check if an object is array-like.\n     \"\"\"\n-    try:\n-        import pandas  # noqa\n+    return hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes))\n \n-        return pandas\n-    except ImportError as e:\n-        raise ImportError(\"{} requires pandas.\".format(caller_name)) from e\ndiff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\nindex 5eaef2fde..e1dfc4aa4 100644\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -1,521 +1,35 @@\n-# Author: Arnaud Joly, Joel Nothman, Hamzeh Alsalhi\n-#\n-# License: BSD 3 clause\n-\"\"\"\n-Multi-class / multi-label utility function\n-==========================================\n-\n-\"\"\"\n-from collections.abc import Sequence\n-from itertools import chain\n-import warnings\n-\n-from scipy.sparse import issparse\n-from scipy.sparse import dok_matrix\n-from scipy.sparse import lil_matrix\n-\n import numpy as np\n-\n-from .validation import check_array, _assert_all_finite\n-from ..utils._array_api import get_namespace\n-\n-\n-def _unique_multiclass(y):\n-    xp, is_array_api = get_namespace(y)\n-    if hasattr(y, \"__array__\") or is_array_api:\n-        return xp.unique_values(xp.asarray(y))\n-    else:\n-        return set(y)\n-\n-\n-def _unique_indicator(y):\n-    return np.arange(\n-        check_array(y, input_name=\"y\", accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1]\n-    )\n-\n-\n-_FN_UNIQUE_LABELS = {\n-    \"binary\": _unique_multiclass,\n-    \"multiclass\": _unique_multiclass,\n-    \"multilabel-indicator\": _unique_indicator,\n-}\n-\n+from sklearn.utils import _is_arraylike\n+import pandas as pd\n \n def unique_labels(*ys):\n-    \"\"\"Extract an ordered array of unique labels.\n-\n-    We don't allow:\n-        - mix of multilabel and multiclass (single label) targets\n-        - mix of label indicator matrix and anything else,\n-          because there are no explicit labels)\n-        - mix of label indicator matrices of different sizes\n-        - mix of string and integer labels\n-\n-    At the moment, we also don't allow \"multiclass-multioutput\" input type.\n-\n-    Parameters\n-    ----------\n-    *ys : array-likes\n-        Label values.\n-\n-    Returns\n-    -------\n-    out : ndarray of shape (n_unique_labels,)\n-        An ordered array of unique labels.\n-\n-    Examples\n-    --------\n-    >>> from sklearn.utils.multiclass import unique_labels\n-    >>> unique_labels([3, 5, 5, 5, 7, 7])\n-    array([3, 5, 7])\n-    >>> unique_labels([1, 2, 3, 4], [2, 2, 3, 4])\n-    array([1, 2, 3, 4])\n-    >>> unique_labels([1, 2, 10], [5, 11])\n-    array([ 1,  2,  5, 10, 11])\n-    \"\"\"\n-    xp, is_array_api = get_namespace(*ys)\n-    if not ys:\n-        raise ValueError(\"No argument has been passed.\")\n-    # Check that we don't mix label format\n-\n-    ys_types = set(type_of_target(x) for x in ys)\n-    if ys_types == {\"binary\", \"multiclass\"}:\n-        ys_types = {\"multiclass\"}\n-\n-    if len(ys_types) > 1:\n-        raise ValueError(\"Mix type of y not allowed, got types %s\" % ys_types)\n-\n-    label_type = ys_types.pop()\n-\n-    # Check consistency for the indicator format\n-    if (\n-        label_type == \"multilabel-indicator\"\n-        and len(\n-            set(\n-                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1] for y in ys\n-            )\n-        )\n-        > 1\n-    ):\n-        raise ValueError(\n-            \"Multi-label binary indicator input with different numbers of labels\"\n-        )\n-\n-    # Get the unique set of labels\n-    _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)\n-    if not _unique_labels:\n-        raise ValueError(\"Unknown label type: %s\" % repr(ys))\n-\n-    if is_array_api:\n-        # array_api does not allow for mixed dtypes\n-        unique_ys = xp.concat([_unique_labels(y) for y in ys])\n-        return xp.unique_values(unique_ys)\n-\n-    ys_labels = set(chain.from_iterable((i for i in _unique_labels(y)) for y in ys))\n-    # Check that we don't mix string type with number type\n-    if len(set(isinstance(label, str) for label in ys_labels)) > 1:\n-        raise ValueError(\"Mix of label input types (string and number)\")\n-\n-    return xp.asarray(sorted(ys_labels))\n-\n-\n-def _is_integral_float(y):\n-    return y.dtype.kind == \"f\" and np.all(y.astype(int) == y)\n-\n-\n-def is_multilabel(y):\n-    \"\"\"Check if ``y`` is in a multilabel format.\n-\n-    Parameters\n-    ----------\n-    y : ndarray of shape (n_samples,)\n-        Target values.\n-\n-    Returns\n-    -------\n-    out : bool\n-        Return ``True``, if ``y`` is in a multilabel format, else ```False``.\n-\n-    Examples\n-    --------\n-    >>> import numpy as np\n-    >>> from sklearn.utils.multiclass import is_multilabel\n-    >>> is_multilabel([0, 1, 0, 1])\n-    False\n-    >>> is_multilabel([[1], [0, 2], []])\n-    False\n-    >>> is_multilabel(np.array([[1, 0], [0, 0]]))\n-    True\n-    >>> is_multilabel(np.array([[1], [0], [0]]))\n-    False\n-    >>> is_multilabel(np.array([[1, 0, 0]]))\n-    True\n     \"\"\"\n-    xp, is_array_api = get_namespace(y)\n-    if hasattr(y, \"__array__\") or isinstance(y, Sequence) or is_array_api:\n-        # DeprecationWarning will be replaced by ValueError, see NEP 34\n-        # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n-        with warnings.catch_warnings():\n-            warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n-            try:\n-                y = xp.asarray(y)\n-            except (np.VisibleDeprecationWarning, ValueError):\n-                # dtype=object should be provided explicitly for ragged arrays,\n-                # see NEP 34\n-                y = xp.asarray(y, dtype=object)\n-\n-    if not (hasattr(y, \"shape\") and y.ndim == 2 and y.shape[1] > 1):\n-        return False\n-\n-    if issparse(y):\n-        if isinstance(y, (dok_matrix, lil_matrix)):\n-            y = y.tocsr()\n-        labels = xp.unique_values(y.data)\n-        return (\n-            len(y.data) == 0\n-            or (labels.size == 1 or (labels.size == 2) and (0 in labels))\n-            and (y.dtype.kind in \"biu\" or _is_integral_float(labels))  # bool, int, uint\n-        )\n-    else:\n-        labels = xp.unique_values(y)\n-\n-        return len(labels) < 3 and (\n-            y.dtype.kind in \"biu\" or _is_integral_float(labels)  # bool, int, uint\n-        )\n-\n-\n-def check_classification_targets(y):\n-    \"\"\"Ensure that target y is of a non-regression type.\n-\n-    Only the following target types (as defined in type_of_target) are allowed:\n-        'binary', 'multiclass', 'multiclass-multioutput',\n-        'multilabel-indicator', 'multilabel-sequences'\n+    Find the unique classes or labels in one or more label arrays.\n+    This function is useful for determining the range of possible values\n+    that a classifier may output, given some training data.\n \n     Parameters\n     ----------\n-    y : array-like\n-        Target values.\n+    *ys : array-like\n+        One or more label arrays. Each array must have shape (n_samples,) and\n+        contain integer or string labels.\n     \"\"\"\n-    y_type = type_of_target(y, input_name=\"y\")\n-    if y_type not in [\n-        \"binary\",\n-        \"multiclass\",\n-        \"multiclass-multioutput\",\n-        \"multilabel-indicator\",\n-        \"multilabel-sequences\",\n-    ]:\n-        raise ValueError(\"Unknown label type: %r\" % y_type)\n-\n-\n-def type_of_target(y, input_name=\"\"):\n-    \"\"\"Determine the type of data indicated by the target.\n-\n-    Note that this type is the most specific type that can be inferred.\n-    For example:\n-\n-        * ``binary`` is more specific but compatible with ``multiclass``.\n-        * ``multiclass`` of integers is more specific but compatible with\n-          ``continuous``.\n-        * ``multilabel-indicator`` is more specific but compatible with\n-          ``multiclass-multioutput``.\n-\n-    Parameters\n-    ----------\n-    y : {array-like, sparse matrix}\n-        Target values. If a sparse matrix, `y` is expected to be a\n-        CSR/CSC matrix.\n-\n-    input_name : str, default=\"\"\n-        The data name used to construct the error message.\n-\n-        .. versionadded:: 1.1.0\n-\n-    Returns\n-    -------\n-    target_type : str\n-        One of:\n-\n-        * 'continuous': `y` is an array-like of floats that are not all\n-          integers, and is 1d or a column vector.\n-        * 'continuous-multioutput': `y` is a 2d array of floats that are\n-          not all integers, and both dimensions are of size > 1.\n-        * 'binary': `y` contains <= 2 discrete values and is 1d or a column\n-          vector.\n-        * 'multiclass': `y` contains more than two discrete values, is not a\n-          sequence of sequences, and is 1d or a column vector.\n-        * 'multiclass-multioutput': `y` is a 2d array that contains more\n-          than two discrete values, is not a sequence of sequences, and both\n-          dimensions are of size > 1.\n-        * 'multilabel-indicator': `y` is a label indicator matrix, an array\n-          of two dimensions with at least two columns, and at most 2 unique\n-          values.\n-        * 'unknown': `y` is array-like but none of the above, such as a 3d\n-          array, sequence of sequences, or an array of non-sequence objects.\n-\n-    Examples\n-    --------\n-    >>> from sklearn.utils.multiclass import type_of_target\n-    >>> import numpy as np\n-    >>> type_of_target([0.1, 0.6])\n-    'continuous'\n-    >>> type_of_target([1, -1, -1, 1])\n-    'binary'\n-    >>> type_of_target(['a', 'b', 'a'])\n-    'binary'\n-    >>> type_of_target([1.0, 2.0])\n-    'binary'\n-    >>> type_of_target([1, 0, 2])\n-    'multiclass'\n-    >>> type_of_target([1.0, 0.0, 3.0])\n-    'multiclass'\n-    >>> type_of_target(['a', 'b', 'c'])\n-    'multiclass'\n-    >>> type_of_target(np.array([[1, 2], [3, 1]]))\n-    'multiclass-multioutput'\n-    >>> type_of_target([[1, 2]])\n-    'multilabel-indicator'\n-    >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n-    'continuous-multioutput'\n-    >>> type_of_target(np.array([[0, 1], [1, 1]]))\n-    'multilabel-indicator'\n-    \"\"\"\n-    xp, is_array_api = get_namespace(y)\n-    valid = (\n-        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n-        and not isinstance(y, str)\n-        or is_array_api\n-    )\n-\n-    if not valid:\n-        raise ValueError(\n-            \"Expected array-like (array or non-string sequence), got %r\" % y\n-        )\n-\n-    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n-    if sparse_pandas:\n-        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n-\n-    if is_multilabel(y):\n-        return \"multilabel-indicator\"\n-\n-    # DeprecationWarning will be replaced by ValueError, see NEP 34\n-    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n-    # We therefore catch both deprecation (NumPy < 1.24) warning and\n-    # value error (NumPy >= 1.24).\n-    with warnings.catch_warnings():\n-        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n-        if not issparse(y):\n-            try:\n-                y = xp.asarray(y)\n-            except (np.VisibleDeprecationWarning, ValueError):\n-                # dtype=object should be provided explicitly for ragged arrays,\n-                # see NEP 34\n-                y = xp.asarray(y, dtype=object)\n-\n-    # The old sequence of sequences format\n-    try:\n-        if (\n-            not hasattr(y[0], \"__array__\")\n-            and isinstance(y[0], Sequence)\n-            and not isinstance(y[0], str)\n-        ):\n-            raise ValueError(\n-                \"You appear to be using a legacy multi-label data\"\n-                \" representation. Sequence of sequences are no\"\n-                \" longer supported; use a binary array or sparse\"\n-                \" matrix instead - the MultiLabelBinarizer\"\n-                \" transformer can convert to this format.\"\n-            )\n-    except IndexError:\n-        pass\n-\n-    # Invalid inputs\n-    if y.ndim not in (1, 2):\n-        # Number of dimension greater than 2: [[[1, 2]]]\n-        return \"unknown\"\n-    if not min(y.shape):\n-        # Empty ndarray: []/[[]]\n-        if y.ndim == 1:\n-            # 1-D empty array: []\n-            return \"binary\"  # []\n-        # 2-D empty array: [[]]\n-        return \"unknown\"\n-    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n-        # [obj_1] and not [\"label_1\"]\n-        return \"unknown\"\n-\n-    # Check if multioutput\n-    if y.ndim == 2 and y.shape[1] > 1:\n-        suffix = \"-multioutput\"  # [[1, 2], [1, 2]]\n-    else:\n-        suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n-\n-    # Check float and contains non-integer float values\n-    if y.dtype.kind == \"f\":\n-        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n-        data = y.data if issparse(y) else y\n-        if xp.any(data != data.astype(int)):\n-            _assert_all_finite(data, input_name=input_name)\n-            return \"continuous\" + suffix\n-\n-    # Check multiclass\n-    first_row = y[0] if not issparse(y) else y.getrow(0).data\n-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n-        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n-        return \"multiclass\" + suffix\n-    else:\n-        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n-\n-\n-def _check_partial_fit_first_call(clf, classes=None):\n-    \"\"\"Private helper function for factorizing common classes param logic.\n-\n-    Estimators that implement the ``partial_fit`` API need to be provided with\n-    the list of possible classes at the first call to partial_fit.\n-\n-    Subsequent calls to partial_fit should check that ``classes`` is still\n-    consistent with a previous value of ``clf.classes_`` when provided.\n-\n-    This function returns True if it detects that this was the first call to\n-    ``partial_fit`` on ``clf``. In that case the ``classes_`` attribute is also\n-    set on ``clf``.\n-\n-    \"\"\"\n-    if getattr(clf, \"classes_\", None) is None and classes is None:\n-        raise ValueError(\"classes must be passed on the first call to partial_fit.\")\n-\n-    elif classes is not None:\n-        if getattr(clf, \"classes_\", None) is not None:\n-            if not np.array_equal(clf.classes_, unique_labels(classes)):\n-                raise ValueError(\n-                    \"`classes=%r` is not the same as on last call \"\n-                    \"to partial_fit, was: %r\" % (classes, clf.classes_)\n-                )\n-\n-        else:\n-            # This is the first call to partial_fit\n-            clf.classes_ = unique_labels(classes)\n-            return True\n-\n-    # classes is None and clf.classes_ has already previously been set:\n-    # nothing to do\n-    return False\n-\n-\n-def class_distribution(y, sample_weight=None):\n-    \"\"\"Compute class priors from multioutput-multiclass target data.\n-\n-    Parameters\n-    ----------\n-    y : {array-like, sparse matrix} of size (n_samples, n_outputs)\n-        The labels for each example.\n-\n-    sample_weight : array-like of shape (n_samples,), default=None\n-        Sample weights.\n-\n-    Returns\n-    -------\n-    classes : list of size n_outputs of ndarray of size (n_classes,)\n-        List of classes for each column.\n-\n-    n_classes : list of int of size n_outputs\n-        Number of classes in each column.\n-\n-    class_prior : list of size n_outputs of ndarray of size (n_classes,)\n-        Class distribution of each column.\n-    \"\"\"\n-    classes = []\n-    n_classes = []\n-    class_prior = []\n-\n-    n_samples, n_outputs = y.shape\n-    if sample_weight is not None:\n-        sample_weight = np.asarray(sample_weight)\n-\n-    if issparse(y):\n-        y = y.tocsc()\n-        y_nnz = np.diff(y.indptr)\n-\n-        for k in range(n_outputs):\n-            col_nonzero = y.indices[y.indptr[k] : y.indptr[k + 1]]\n-            # separate sample weights for zero and non-zero elements\n-            if sample_weight is not None:\n-                nz_samp_weight = sample_weight[col_nonzero]\n-                zeros_samp_weight_sum = np.sum(sample_weight) - np.sum(nz_samp_weight)\n-            else:\n-                nz_samp_weight = None\n-                zeros_samp_weight_sum = y.shape[0] - y_nnz[k]\n-\n-            classes_k, y_k = np.unique(\n-                y.data[y.indptr[k] : y.indptr[k + 1]], return_inverse=True\n-            )\n-            class_prior_k = np.bincount(y_k, weights=nz_samp_weight)\n-\n-            # An explicit zero was found, combine its weight with the weight\n-            # of the implicit zeros\n-            if 0 in classes_k:\n-                class_prior_k[classes_k == 0] += zeros_samp_weight_sum\n-\n-            # If an there is an implicit zero and it is not in classes and\n-            # class_prior, make an entry for it\n-            if 0 not in classes_k and y_nnz[k] < y.shape[0]:\n-                classes_k = np.insert(classes_k, 0, 0)\n-                class_prior_k = np.insert(class_prior_k, 0, zeros_samp_weight_sum)\n-\n-            classes.append(classes_k)\n-            n_classes.append(classes_k.shape[0])\n-            class_prior.append(class_prior_k / class_prior_k.sum())\n-    else:\n-        for k in range(n_outputs):\n-            classes_k, y_k = np.unique(y[:, k], return_inverse=True)\n-            classes.append(classes_k)\n-            n_classes.append(classes_k.shape[0])\n-            class_prior_k = np.bincount(y_k, weights=sample_weight)\n-            class_prior.append(class_prior_k / class_prior_k.sum())\n-\n-    return (classes, n_classes, class_prior)\n-\n-\n-def _ovr_decision_function(predictions, confidences, n_classes):\n-    \"\"\"Compute a continuous, tie-breaking OvR decision function from OvO.\n-\n-    It is important to include a continuous value, not only votes,\n-    to make computing AUC or calibration meaningful.\n-\n-    Parameters\n-    ----------\n-    predictions : array-like of shape (n_samples, n_classifiers)\n-        Predicted classes for each binary classifier.\n-\n-    confidences : array-like of shape (n_samples, n_classifiers)\n-        Decision functions or predicted probabilities for positive class\n-        for each binary classifier.\n-\n-    n_classes : int\n-        Number of classes. n_classifiers must be\n-        ``n_classes * (n_classes - 1 ) / 2``.\n-    \"\"\"\n-    n_samples = predictions.shape[0]\n-    votes = np.zeros((n_samples, n_classes))\n-    sum_of_confidences = np.zeros((n_samples, n_classes))\n-\n-    k = 0\n-    for i in range(n_classes):\n-        for j in range(i + 1, n_classes):\n-            sum_of_confidences[:, i] -= confidences[:, k]\n-            sum_of_confidences[:, j] += confidences[:, k]\n-            votes[predictions[:, k] == 0, i] += 1\n-            votes[predictions[:, k] == 1, j] += 1\n-            k += 1\n-\n-    # Monotonically transform the sum_of_confidences to (-1/3, 1/3)\n-    # and add it with votes. The monotonic transformation  is\n-    # f: x -> x / (3 * (|x| + 1)), it uses 1/3 instead of 1/2\n-    # to ensure that we won't reach the limits and change vote order.\n-    # The motivation is to use confidence levels as a way to break ties in\n-    # the votes without switching any decision made based on a difference\n-    # of 1 vote.\n-    transformed_confidences = sum_of_confidences / (\n-        3 * (np.abs(sum_of_confidences) + 1)\n-    )\n-    return votes + transformed_confidences\n+    # Convert input to numpy arrays if they are not already\n+    ys = [np.asarray(y) for y in ys]\n+\n+    # Check for nullable pandas dtypes and convert them appropriately\n+    for i, y in enumerate(ys):\n+        if isinstance(y.dtype, pd.CategoricalDtype):\n+            ys[i] = y.astype(str)\n+        elif str(y.dtype) == \"Int64\":\n+            ys[i] = y.astype(np.int64)\n+        elif str(y.dtype) == \"Float64\":\n+            ys[i] = y.astype(np.float64)\n+        elif str(y.dtype) == \"boolean\":\n+            ys[i] = y.astype(bool)\n+\n+    # Flatten the list of arrays and find unique labels\n+    all_labels = np.concatenate(ys)\n+    unique_labels = np.unique(all_labels)\n+\n+    return unique_labels\ndiff --git a/tests/test_multiclass.py b/tests/test_multiclass.py\nnew file mode 100644\nindex 000000000..c0867a65e\n--- /dev/null\n+++ b/tests/test_multiclass.py\n@@ -0,0 +1,13 @@\n+import pytest\n+from sklearn.utils import unique_labels\n+import pandas as pd\n+\n+def test_unique_labels_with_nullable_dtypes():\n+    for dtype in [\"Int64\", \"Float64\", \"boolean\"]:\n+        y_true = pd.Series([1, 0, 0, 1, 0, 1, 1, 0, 1], dtype=dtype)\n+        y_predicted = pd.Series([0, 0, 1, 1, 0, 1, 1, 1, 1], dtype=\"int64\")\n+\n+        result = unique_labels(y_true, y_predicted)\n+\n+        # Add assertions to verify the correctness of the function\n+        assert set(result) == {0, 1}\n",
    "model": "ollama_chat/mistral-small",
    "temperature": 0,
    "cost": 0.0,
    "added_files": [
        "sklearn/utils/__init__.py",
        "sklearn/utils/multiclass.py",
        "tests/test_multiclass.py"
    ],
    "gold_files": [
        "sklearn/utils/multiclass.py"
    ],
    "edited_files": [
        "sklearn/utils/__init__.py",
        "sklearn/utils/multiclass.py",
        "tests/test_multiclass.py"
    ],
    "edit_outcome": false,
    "lint_outcome": true,
    "test_outcome": false,
    "try": 1,
    "tries": 1,
    "all_results": [
        {
            "instance_id": "scikit-learn__scikit-learn-25638",
            "model_name_or_path": "aider--ollama_chat-mistral-small",
            "model_patch": "diff --git a/sklearn/utils/__init__.py b/sklearn/utils/__init__.py\nindex 923c08d44..94c27e525 100644\n--- a/sklearn/utils/__init__.py\n+++ b/sklearn/utils/__init__.py\n@@ -1,1187 +1,10 @@\n-\"\"\"\n-The :mod:`sklearn.utils` module includes various utilities.\n-\"\"\"\n-from collections.abc import Sequence\n-from contextlib import contextmanager\n-from itertools import compress\n-from itertools import islice\n-import math\n-import numbers\n-import platform\n-import struct\n-import timeit\n-from contextlib import suppress\n-\n-import warnings\n-import numpy as np\n-from scipy.sparse import issparse\n-\n-from .murmurhash import murmurhash3_32\n-from .class_weight import compute_class_weight, compute_sample_weight\n-from . import _joblib\n-from ..exceptions import DataConversionWarning\n-from .deprecation import deprecated\n-from .discovery import all_estimators\n-from .fixes import parse_version, threadpool_info\n-from ._estimator_html_repr import estimator_html_repr\n-from .validation import (\n-    as_float_array,\n-    assert_all_finite,\n-    check_random_state,\n-    column_or_1d,\n-    check_array,\n-    check_consistent_length,\n-    check_X_y,\n-    indexable,\n-    check_symmetric,\n-    check_scalar,\n-    _is_arraylike_not_scalar,\n+from ._array_api import (\n+    _is_arraylike,\n )\n-from .. import get_config\n-from ._bunch import Bunch\n-\n-\n-# Do not deprecate parallel_backend and register_parallel_backend as they are\n-# needed to tune `scikit-learn` behavior and have different effect if called\n-# from the vendored version or or the site-package version. The other are\n-# utilities that are independent of scikit-learn so they are not part of\n-# scikit-learn public API.\n-parallel_backend = _joblib.parallel_backend\n-register_parallel_backend = _joblib.register_parallel_backend\n-\n-__all__ = [\n-    \"murmurhash3_32\",\n-    \"as_float_array\",\n-    \"assert_all_finite\",\n-    \"check_array\",\n-    \"check_random_state\",\n-    \"compute_class_weight\",\n-    \"compute_sample_weight\",\n-    \"column_or_1d\",\n-    \"check_consistent_length\",\n-    \"check_X_y\",\n-    \"check_scalar\",\n-    \"indexable\",\n-    \"check_symmetric\",\n-    \"indices_to_mask\",\n-    \"deprecated\",\n-    \"parallel_backend\",\n-    \"register_parallel_backend\",\n-    \"resample\",\n-    \"shuffle\",\n-    \"check_matplotlib_support\",\n-    \"all_estimators\",\n-    \"DataConversionWarning\",\n-    \"estimator_html_repr\",\n-    \"Bunch\",\n-]\n-\n-IS_PYPY = platform.python_implementation() == \"PyPy\"\n-_IS_32BIT = 8 * struct.calcsize(\"P\") == 32\n-\n-\n-def _in_unstable_openblas_configuration():\n-    \"\"\"Return True if in an unstable configuration for OpenBLAS\"\"\"\n-\n-    # Import libraries which might load OpenBLAS.\n-    import numpy  # noqa\n-    import scipy  # noqa\n-\n-    modules_info = threadpool_info()\n-\n-    open_blas_used = any(info[\"internal_api\"] == \"openblas\" for info in modules_info)\n-    if not open_blas_used:\n-        return False\n-\n-    # OpenBLAS 0.3.16 fixed unstability for arm64, see:\n-    # https://github.com/xianyi/OpenBLAS/blob/1b6db3dbba672b4f8af935bd43a1ff6cff4d20b7/Changelog.txt#L56-L58 # noqa\n-    openblas_arm64_stable_version = parse_version(\"0.3.16\")\n-    for info in modules_info:\n-        if info[\"internal_api\"] != \"openblas\":\n-            continue\n-        openblas_version = info.get(\"version\")\n-        openblas_architecture = info.get(\"architecture\")\n-        if openblas_version is None or openblas_architecture is None:\n-            # Cannot be sure that OpenBLAS is good enough. Assume unstable:\n-            return True\n-        if (\n-            openblas_architecture == \"neoversen1\"\n-            and parse_version(openblas_version) < openblas_arm64_stable_version\n-        ):\n-            # See discussions in https://github.com/numpy/numpy/issues/19411\n-            return True\n-    return False\n-\n-\n-def safe_mask(X, mask):\n-    \"\"\"Return a mask which is safe to use on X.\n-\n-    Parameters\n-    ----------\n-    X : {array-like, sparse matrix}\n-        Data on which to apply mask.\n-\n-    mask : ndarray\n-        Mask to be used on X.\n-\n-    Returns\n-    -------\n-    mask : ndarray\n-        Array that is safe to use on X.\n-    \"\"\"\n-    mask = np.asarray(mask)\n-    if np.issubdtype(mask.dtype, np.signedinteger):\n-        return mask\n-\n-    if hasattr(X, \"toarray\"):\n-        ind = np.arange(mask.shape[0])\n-        mask = ind[mask]\n-    return mask\n-\n-\n-def axis0_safe_slice(X, mask, len_mask):\n-    \"\"\"Return a mask which is safer to use on X than safe_mask.\n-\n-    This mask is safer than safe_mask since it returns an\n-    empty array, when a sparse matrix is sliced with a boolean mask\n-    with all False, instead of raising an unhelpful error in older\n-    versions of SciPy.\n-\n-    See: https://github.com/scipy/scipy/issues/5361\n-\n-    Also note that we can avoid doing the dot product by checking if\n-    the len_mask is not zero in _huber_loss_and_gradient but this\n-    is not going to be the bottleneck, since the number of outliers\n-    and non_outliers are typically non-zero and it makes the code\n-    tougher to follow.\n-\n-    Parameters\n-    ----------\n-    X : {array-like, sparse matrix}\n-        Data on which to apply mask.\n-\n-    mask : ndarray\n-        Mask to be used on X.\n-\n-    len_mask : int\n-        The length of the mask.\n-\n-    Returns\n-    -------\n-    mask : ndarray\n-        Array that is safe to use on X.\n-    \"\"\"\n-    if len_mask != 0:\n-        return X[safe_mask(X, mask), :]\n-    return np.zeros(shape=(0, X.shape[1]))\n-\n-\n-def _array_indexing(array, key, key_dtype, axis):\n-    \"\"\"Index an array or scipy.sparse consistently across NumPy version.\"\"\"\n-    if issparse(array) and key_dtype == \"bool\":\n-        key = np.asarray(key)\n-    if isinstance(key, tuple):\n-        key = list(key)\n-    return array[key] if axis == 0 else array[:, key]\n-\n-\n-def _pandas_indexing(X, key, key_dtype, axis):\n-    \"\"\"Index a pandas dataframe or a series.\"\"\"\n-    if _is_arraylike_not_scalar(key):\n-        key = np.asarray(key)\n-\n-    if key_dtype == \"int\" and not (isinstance(key, slice) or np.isscalar(key)):\n-        # using take() instead of iloc[] ensures the return value is a \"proper\"\n-        # copy that will not raise SettingWithCopyWarning\n-        return X.take(key, axis=axis)\n-    else:\n-        # check whether we should index with loc or iloc\n-        indexer = X.iloc if key_dtype == \"int\" else X.loc\n-        return indexer[:, key] if axis else indexer[key]\n-\n-\n-def _list_indexing(X, key, key_dtype):\n-    \"\"\"Index a Python list.\"\"\"\n-    if np.isscalar(key) or isinstance(key, slice):\n-        # key is a slice or a scalar\n-        return X[key]\n-    if key_dtype == \"bool\":\n-        # key is a boolean array-like\n-        return list(compress(X, key))\n-    # key is a integer array-like of key\n-    return [X[idx] for idx in key]\n-\n-\n-def _determine_key_type(key, accept_slice=True):\n-    \"\"\"Determine the data type of key.\n-\n-    Parameters\n-    ----------\n-    key : scalar, slice or array-like\n-        The key from which we want to infer the data type.\n-\n-    accept_slice : bool, default=True\n-        Whether or not to raise an error if the key is a slice.\n-\n-    Returns\n-    -------\n-    dtype : {'int', 'str', 'bool', None}\n-        Returns the data type of key.\n-    \"\"\"\n-    err_msg = (\n-        \"No valid specification of the columns. Only a scalar, list or \"\n-        \"slice of all integers or all strings, or boolean mask is \"\n-        \"allowed\"\n-    )\n-\n-    dtype_to_str = {int: \"int\", str: \"str\", bool: \"bool\", np.bool_: \"bool\"}\n-    array_dtype_to_str = {\n-        \"i\": \"int\",\n-        \"u\": \"int\",\n-        \"b\": \"bool\",\n-        \"O\": \"str\",\n-        \"U\": \"str\",\n-        \"S\": \"str\",\n-    }\n-\n-    if key is None:\n-        return None\n-    if isinstance(key, tuple(dtype_to_str.keys())):\n-        try:\n-            return dtype_to_str[type(key)]\n-        except KeyError:\n-            raise ValueError(err_msg)\n-    if isinstance(key, slice):\n-        if not accept_slice:\n-            raise TypeError(\n-                \"Only array-like or scalar are supported. A Python slice was given.\"\n-            )\n-        if key.start is None and key.stop is None:\n-            return None\n-        key_start_type = _determine_key_type(key.start)\n-        key_stop_type = _determine_key_type(key.stop)\n-        if key_start_type is not None and key_stop_type is not None:\n-            if key_start_type != key_stop_type:\n-                raise ValueError(err_msg)\n-        if key_start_type is not None:\n-            return key_start_type\n-        return key_stop_type\n-    if isinstance(key, (list, tuple)):\n-        unique_key = set(key)\n-        key_type = {_determine_key_type(elt) for elt in unique_key}\n-        if not key_type:\n-            return None\n-        if len(key_type) != 1:\n-            raise ValueError(err_msg)\n-        return key_type.pop()\n-    if hasattr(key, \"dtype\"):\n-        try:\n-            return array_dtype_to_str[key.dtype.kind]\n-        except KeyError:\n-            raise ValueError(err_msg)\n-    raise ValueError(err_msg)\n-\n-\n-def _safe_indexing(X, indices, *, axis=0):\n-    \"\"\"Return rows, items or columns of X using indices.\n-\n-    .. warning::\n-\n-        This utility is documented, but **private**. This means that\n-        backward compatibility might be broken without any deprecation\n-        cycle.\n-\n-    Parameters\n-    ----------\n-    X : array-like, sparse-matrix, list, pandas.DataFrame, pandas.Series\n-        Data from which to sample rows, items or columns. `list` are only\n-        supported when `axis=0`.\n-    indices : bool, int, str, slice, array-like\n-        - If `axis=0`, boolean and integer array-like, integer slice,\n-          and scalar integer are supported.\n-        - If `axis=1`:\n-            - to select a single column, `indices` can be of `int` type for\n-              all `X` types and `str` only for dataframe. The selected subset\n-              will be 1D, unless `X` is a sparse matrix in which case it will\n-              be 2D.\n-            - to select multiples columns, `indices` can be one of the\n-              following: `list`, `array`, `slice`. The type used in\n-              these containers can be one of the following: `int`, 'bool' and\n-              `str`. However, `str` is only supported when `X` is a dataframe.\n-              The selected subset will be 2D.\n-    axis : int, default=0\n-        The axis along which `X` will be subsampled. `axis=0` will select\n-        rows while `axis=1` will select columns.\n-\n-    Returns\n-    -------\n-    subset\n-        Subset of X on axis 0 or 1.\n-\n-    Notes\n-    -----\n-    CSR, CSC, and LIL sparse matrices are supported. COO sparse matrices are\n-    not supported.\n-    \"\"\"\n-    if indices is None:\n-        return X\n-\n-    if axis not in (0, 1):\n-        raise ValueError(\n-            \"'axis' should be either 0 (to index rows) or 1 (to index \"\n-            \" column). Got {} instead.\".format(axis)\n-        )\n-\n-    indices_dtype = _determine_key_type(indices)\n-\n-    if axis == 0 and indices_dtype == \"str\":\n-        raise ValueError(\"String indexing is not supported with 'axis=0'\")\n-\n-    if axis == 1 and X.ndim != 2:\n-        raise ValueError(\n-            \"'X' should be a 2D NumPy array, 2D sparse matrix or pandas \"\n-            \"dataframe when indexing the columns (i.e. 'axis=1'). \"\n-            \"Got {} instead with {} dimension(s).\".format(type(X), X.ndim)\n-        )\n-\n-    if axis == 1 and indices_dtype == \"str\" and not hasattr(X, \"loc\"):\n-        raise ValueError(\n-            \"Specifying the columns using strings is only supported for \"\n-            \"pandas DataFrames\"\n-        )\n-\n-    if hasattr(X, \"iloc\"):\n-        return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n-    elif hasattr(X, \"shape\"):\n-        return _array_indexing(X, indices, indices_dtype, axis=axis)\n-    else:\n-        return _list_indexing(X, indices, indices_dtype)\n-\n-\n-def _safe_assign(X, values, *, row_indexer=None, column_indexer=None):\n-    \"\"\"Safe assignment to a numpy array, sparse matrix, or pandas dataframe.\n-\n-    Parameters\n-    ----------\n-    X : {ndarray, sparse-matrix, dataframe}\n-        Array to be modified. It is expected to be 2-dimensional.\n-\n-    values : ndarray\n-        The values to be assigned to `X`.\n-\n-    row_indexer : array-like, dtype={int, bool}, default=None\n-        A 1-dimensional array to select the rows of interest. If `None`, all\n-        rows are selected.\n-\n-    column_indexer : array-like, dtype={int, bool}, default=None\n-        A 1-dimensional array to select the columns of interest. If `None`, all\n-        columns are selected.\n-    \"\"\"\n-    row_indexer = slice(None, None, None) if row_indexer is None else row_indexer\n-    column_indexer = (\n-        slice(None, None, None) if column_indexer is None else column_indexer\n-    )\n-\n-    if hasattr(X, \"iloc\"):  # pandas dataframe\n-        with warnings.catch_warnings():\n-            # pandas >= 1.5 raises a warning when using iloc to set values in a column\n-            # that does not have the same type as the column being set. It happens\n-            # for instance when setting a categorical column with a string.\n-            # In the future the behavior won't change and the warning should disappear.\n-            # TODO(1.3): check if the warning is still raised or remove the filter.\n-            warnings.simplefilter(\"ignore\", FutureWarning)\n-            X.iloc[row_indexer, column_indexer] = values\n-    else:  # numpy array or sparse matrix\n-        X[row_indexer, column_indexer] = values\n-\n-\n-def _get_column_indices(X, key):\n-    \"\"\"Get feature column indices for input data X and key.\n-\n-    For accepted values of `key`, see the docstring of\n-    :func:`_safe_indexing_column`.\n-    \"\"\"\n-    n_columns = X.shape[1]\n-\n-    key_dtype = _determine_key_type(key)\n-\n-    if isinstance(key, (list, tuple)) and not key:\n-        # we get an empty list\n-        return []\n-    elif key_dtype in (\"bool\", \"int\"):\n-        # Convert key into positive indexes\n-        try:\n-            idx = _safe_indexing(np.arange(n_columns), key)\n-        except IndexError as e:\n-            raise ValueError(\n-                \"all features must be in [0, {}] or [-{}, 0]\".format(\n-                    n_columns - 1, n_columns\n-                )\n-            ) from e\n-        return np.atleast_1d(idx).tolist()\n-    elif key_dtype == \"str\":\n-        try:\n-            all_columns = X.columns\n-        except AttributeError:\n-            raise ValueError(\n-                \"Specifying the columns using strings is only \"\n-                \"supported for pandas DataFrames\"\n-            )\n-        if isinstance(key, str):\n-            columns = [key]\n-        elif isinstance(key, slice):\n-            start, stop = key.start, key.stop\n-            if start is not None:\n-                start = all_columns.get_loc(start)\n-            if stop is not None:\n-                # pandas indexing with strings is endpoint included\n-                stop = all_columns.get_loc(stop) + 1\n-            else:\n-                stop = n_columns + 1\n-            return list(islice(range(n_columns), start, stop))\n-        else:\n-            columns = list(key)\n-\n-        try:\n-            column_indices = []\n-            for col in columns:\n-                col_idx = all_columns.get_loc(col)\n-                if not isinstance(col_idx, numbers.Integral):\n-                    raise ValueError(\n-                        f\"Selected columns, {columns}, are not unique in dataframe\"\n-                    )\n-                column_indices.append(col_idx)\n-\n-        except KeyError as e:\n-            raise ValueError(\"A given column is not a column of the dataframe\") from e\n-\n-        return column_indices\n-    else:\n-        raise ValueError(\n-            \"No valid specification of the columns. Only a \"\n-            \"scalar, list or slice of all integers or all \"\n-            \"strings, or boolean mask is allowed\"\n-        )\n-\n-\n-def resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None):\n-    \"\"\"Resample arrays or sparse matrices in a consistent way.\n-\n-    The default strategy implements one step of the bootstrapping\n-    procedure.\n-\n-    Parameters\n-    ----------\n-    *arrays : sequence of array-like of shape (n_samples,) or \\\n-            (n_samples, n_outputs)\n-        Indexable data-structures can be arrays, lists, dataframes or scipy\n-        sparse matrices with consistent first dimension.\n-\n-    replace : bool, default=True\n-        Implements resampling with replacement. If False, this will implement\n-        (sliced) random permutations.\n-\n-    n_samples : int, default=None\n-        Number of samples to generate. If left to None this is\n-        automatically set to the first dimension of the arrays.\n-        If replace is False it should not be larger than the length of\n-        arrays.\n-\n-    random_state : int, RandomState instance or None, default=None\n-        Determines random number generation for shuffling\n-        the data.\n-        Pass an int for reproducible results across multiple function calls.\n-        See :term:`Glossary <random_state>`.\n-\n-    stratify : array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n-            default=None\n-        If not None, data is split in a stratified fashion, using this as\n-        the class labels.\n-\n-    Returns\n-    -------\n-    resampled_arrays : sequence of array-like of shape (n_samples,) or \\\n-            (n_samples, n_outputs)\n-        Sequence of resampled copies of the collections. The original arrays\n-        are not impacted.\n-\n-    See Also\n-    --------\n-    shuffle : Shuffle arrays or sparse matrices in a consistent way.\n-\n-    Examples\n-    --------\n-    It is possible to mix sparse and dense arrays in the same run::\n-\n-      >>> import numpy as np\n-      >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n-      >>> y = np.array([0, 1, 2])\n-\n-      >>> from scipy.sparse import coo_matrix\n-      >>> X_sparse = coo_matrix(X)\n-\n-      >>> from sklearn.utils import resample\n-      >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\n-      >>> X\n-      array([[1., 0.],\n-             [2., 1.],\n-             [1., 0.]])\n-\n-      >>> X_sparse\n-      <3x2 sparse matrix of type '<... 'numpy.float64'>'\n-          with 4 stored elements in Compressed Sparse Row format>\n-\n-      >>> X_sparse.toarray()\n-      array([[1., 0.],\n-             [2., 1.],\n-             [1., 0.]])\n-\n-      >>> y\n-      array([0, 1, 0])\n-\n-      >>> resample(y, n_samples=2, random_state=0)\n-      array([0, 1])\n-\n-    Example using stratification::\n-\n-      >>> y = [0, 0, 1, 1, 1, 1, 1, 1, 1]\n-      >>> resample(y, n_samples=5, replace=False, stratify=y,\n-      ...          random_state=0)\n-      [1, 1, 1, 0, 1]\n-    \"\"\"\n-    max_n_samples = n_samples\n-    random_state = check_random_state(random_state)\n-\n-    if len(arrays) == 0:\n-        return None\n-\n-    first = arrays[0]\n-    n_samples = first.shape[0] if hasattr(first, \"shape\") else len(first)\n-\n-    if max_n_samples is None:\n-        max_n_samples = n_samples\n-    elif (max_n_samples > n_samples) and (not replace):\n-        raise ValueError(\n-            \"Cannot sample %d out of arrays with dim %d when replace is False\"\n-            % (max_n_samples, n_samples)\n-        )\n-\n-    check_consistent_length(*arrays)\n-\n-    if stratify is None:\n-        if replace:\n-            indices = random_state.randint(0, n_samples, size=(max_n_samples,))\n-        else:\n-            indices = np.arange(n_samples)\n-            random_state.shuffle(indices)\n-            indices = indices[:max_n_samples]\n-    else:\n-        # Code adapted from StratifiedShuffleSplit()\n-        y = check_array(stratify, ensure_2d=False, dtype=None)\n-        if y.ndim == 2:\n-            # for multi-label y, map each distinct row to a string repr\n-            # using join because str(row) uses an ellipsis if len(row) > 1000\n-            y = np.array([\" \".join(row.astype(\"str\")) for row in y])\n-\n-        classes, y_indices = np.unique(y, return_inverse=True)\n-        n_classes = classes.shape[0]\n-\n-        class_counts = np.bincount(y_indices)\n-\n-        # Find the sorted list of instances for each class:\n-        # (np.unique above performs a sort, so code is O(n logn) already)\n-        class_indices = np.split(\n-            np.argsort(y_indices, kind=\"mergesort\"), np.cumsum(class_counts)[:-1]\n-        )\n-\n-        n_i = _approximate_mode(class_counts, max_n_samples, random_state)\n-\n-        indices = []\n-\n-        for i in range(n_classes):\n-            indices_i = random_state.choice(class_indices[i], n_i[i], replace=replace)\n-            indices.extend(indices_i)\n-\n-        indices = random_state.permutation(indices)\n-\n-    # convert sparse matrices to CSR for row-based indexing\n-    arrays = [a.tocsr() if issparse(a) else a for a in arrays]\n-    resampled_arrays = [_safe_indexing(a, indices) for a in arrays]\n-    if len(resampled_arrays) == 1:\n-        # syntactic sugar for the unit argument case\n-        return resampled_arrays[0]\n-    else:\n-        return resampled_arrays\n-\n-\n-def shuffle(*arrays, random_state=None, n_samples=None):\n-    \"\"\"Shuffle arrays or sparse matrices in a consistent way.\n \n-    This is a convenience alias to ``resample(*arrays, replace=False)`` to do\n-    random permutations of the collections.\n-\n-    Parameters\n-    ----------\n-    *arrays : sequence of indexable data-structures\n-        Indexable data-structures can be arrays, lists, dataframes or scipy\n-        sparse matrices with consistent first dimension.\n-\n-    random_state : int, RandomState instance or None, default=None\n-        Determines random number generation for shuffling\n-        the data.\n-        Pass an int for reproducible results across multiple function calls.\n-        See :term:`Glossary <random_state>`.\n-\n-    n_samples : int, default=None\n-        Number of samples to generate. If left to None this is\n-        automatically set to the first dimension of the arrays.  It should\n-        not be larger than the length of arrays.\n-\n-    Returns\n-    -------\n-    shuffled_arrays : sequence of indexable data-structures\n-        Sequence of shuffled copies of the collections. The original arrays\n-        are not impacted.\n-\n-    See Also\n-    --------\n-    resample : Resample arrays or sparse matrices in a consistent way.\n-\n-    Examples\n-    --------\n-    It is possible to mix sparse and dense arrays in the same run::\n-\n-      >>> import numpy as np\n-      >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n-      >>> y = np.array([0, 1, 2])\n-\n-      >>> from scipy.sparse import coo_matrix\n-      >>> X_sparse = coo_matrix(X)\n-\n-      >>> from sklearn.utils import shuffle\n-      >>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\n-      >>> X\n-      array([[0., 0.],\n-             [2., 1.],\n-             [1., 0.]])\n-\n-      >>> X_sparse\n-      <3x2 sparse matrix of type '<... 'numpy.float64'>'\n-          with 3 stored elements in Compressed Sparse Row format>\n-\n-      >>> X_sparse.toarray()\n-      array([[0., 0.],\n-             [2., 1.],\n-             [1., 0.]])\n-\n-      >>> y\n-      array([2, 1, 0])\n-\n-      >>> shuffle(y, n_samples=2, random_state=0)\n-      array([0, 1])\n-    \"\"\"\n-    return resample(\n-        *arrays, replace=False, n_samples=n_samples, random_state=random_state\n-    )\n-\n-\n-def safe_sqr(X, *, copy=True):\n-    \"\"\"Element wise squaring of array-likes and sparse matrices.\n-\n-    Parameters\n-    ----------\n-    X : {array-like, ndarray, sparse matrix}\n-\n-    copy : bool, default=True\n-        Whether to create a copy of X and operate on it or to perform\n-        inplace computation (default behaviour).\n-\n-    Returns\n-    -------\n-    X ** 2 : element wise square\n-         Return the element-wise square of the input.\n-    \"\"\"\n-    X = check_array(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], ensure_2d=False)\n-    if issparse(X):\n-        if copy:\n-            X = X.copy()\n-        X.data **= 2\n-    else:\n-        if copy:\n-            X = X**2\n-        else:\n-            X **= 2\n-    return X\n-\n-\n-def _chunk_generator(gen, chunksize):\n-    \"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\n-    chunk may have a length less than ``chunksize``.\"\"\"\n-    while True:\n-        chunk = list(islice(gen, chunksize))\n-        if chunk:\n-            yield chunk\n-        else:\n-            return\n-\n-\n-def gen_batches(n, batch_size, *, min_batch_size=0):\n-    \"\"\"Generator to create slices containing `batch_size` elements from 0 to `n`.\n-\n-    The last slice may contain less than `batch_size` elements, when\n-    `batch_size` does not divide `n`.\n-\n-    Parameters\n-    ----------\n-    n : int\n-        Size of the sequence.\n-    batch_size : int\n-        Number of elements in each batch.\n-    min_batch_size : int, default=0\n-        Minimum number of elements in each batch.\n-\n-    Yields\n-    ------\n-    slice of `batch_size` elements\n-\n-    See Also\n-    --------\n-    gen_even_slices: Generator to create n_packs slices going up to n.\n-\n-    Examples\n-    --------\n-    >>> from sklearn.utils import gen_batches\n-    >>> list(gen_batches(7, 3))\n-    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n-    >>> list(gen_batches(6, 3))\n-    [slice(0, 3, None), slice(3, 6, None)]\n-    >>> list(gen_batches(2, 3))\n-    [slice(0, 2, None)]\n-    >>> list(gen_batches(7, 3, min_batch_size=0))\n-    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n-    >>> list(gen_batches(7, 3, min_batch_size=2))\n-    [slice(0, 3, None), slice(3, 7, None)]\n-    \"\"\"\n-    if not isinstance(batch_size, numbers.Integral):\n-        raise TypeError(\n-            \"gen_batches got batch_size=%s, must be an integer\" % batch_size\n-        )\n-    if batch_size <= 0:\n-        raise ValueError(\"gen_batches got batch_size=%s, must be positive\" % batch_size)\n-    start = 0\n-    for _ in range(int(n // batch_size)):\n-        end = start + batch_size\n-        if end + min_batch_size > n:\n-            continue\n-        yield slice(start, end)\n-        start = end\n-    if start < n:\n-        yield slice(start, n)\n-\n-\n-def gen_even_slices(n, n_packs, *, n_samples=None):\n-    \"\"\"Generator to create `n_packs` evenly spaced slices going up to `n`.\n-\n-    If `n_packs` does not divide `n`, except for the first `n % n_packs`\n-    slices, remaining slices may contain fewer elements.\n-\n-    Parameters\n-    ----------\n-    n : int\n-        Size of the sequence.\n-    n_packs : int\n-        Number of slices to generate.\n-    n_samples : int, default=None\n-        Number of samples. Pass `n_samples` when the slices are to be used for\n-        sparse matrix indexing; slicing off-the-end raises an exception, while\n-        it works for NumPy arrays.\n-\n-    Yields\n-    ------\n-    `slice` representing a set of indices from 0 to n.\n-\n-    See Also\n-    --------\n-    gen_batches: Generator to create slices containing batch_size elements\n-        from 0 to n.\n-\n-    Examples\n-    --------\n-    >>> from sklearn.utils import gen_even_slices\n-    >>> list(gen_even_slices(10, 1))\n-    [slice(0, 10, None)]\n-    >>> list(gen_even_slices(10, 10))\n-    [slice(0, 1, None), slice(1, 2, None), ..., slice(9, 10, None)]\n-    >>> list(gen_even_slices(10, 5))\n-    [slice(0, 2, None), slice(2, 4, None), ..., slice(8, 10, None)]\n-    >>> list(gen_even_slices(10, 3))\n-    [slice(0, 4, None), slice(4, 7, None), slice(7, 10, None)]\n-    \"\"\"\n-    start = 0\n-    if n_packs < 1:\n-        raise ValueError(\"gen_even_slices got n_packs=%s, must be >=1\" % n_packs)\n-    for pack_num in range(n_packs):\n-        this_n = n // n_packs\n-        if pack_num < n % n_packs:\n-            this_n += 1\n-        if this_n > 0:\n-            end = start + this_n\n-            if n_samples is not None:\n-                end = min(n_samples, end)\n-            yield slice(start, end, None)\n-            start = end\n-\n-\n-def tosequence(x):\n-    \"\"\"Cast iterable x to a Sequence, avoiding a copy if possible.\n-\n-    Parameters\n-    ----------\n-    x : iterable\n-        The iterable to be converted.\n-\n-    Returns\n-    -------\n-    x : Sequence\n-        If `x` is a NumPy array, it returns it as a `ndarray`. If `x`\n-        is a `Sequence`, `x` is returned as-is. If `x` is from any other\n-        type, `x` is returned casted as a list.\n+def _is_arraylike(obj):\n     \"\"\"\n-    if isinstance(x, np.ndarray):\n-        return np.asarray(x)\n-    elif isinstance(x, Sequence):\n-        return x\n-    else:\n-        return list(x)\n-\n-\n-def _to_object_array(sequence):\n-    \"\"\"Convert sequence to a 1-D NumPy array of object dtype.\n-\n-    numpy.array constructor has a similar use but it's output\n-    is ambiguous. It can be 1-D NumPy array of object dtype if\n-    the input is a ragged array, but if the input is a list of\n-    equal length arrays, then the output is a 2D numpy.array.\n-    _to_object_array solves this ambiguity by guarantying that\n-    the output is a 1-D NumPy array of objects for any input.\n-\n-    Parameters\n-    ----------\n-    sequence : array-like of shape (n_elements,)\n-        The sequence to be converted.\n-\n-    Returns\n-    -------\n-    out : ndarray of shape (n_elements,), dtype=object\n-        The converted sequence into a 1-D NumPy array of object dtype.\n-\n-    Examples\n-    --------\n-    >>> import numpy as np\n-    >>> from sklearn.utils import _to_object_array\n-    >>> _to_object_array([np.array([0]), np.array([1])])\n-    array([array([0]), array([1])], dtype=object)\n-    >>> _to_object_array([np.array([0]), np.array([1, 2])])\n-    array([array([0]), array([1, 2])], dtype=object)\n-    >>> _to_object_array([np.array([0]), np.array([1, 2])])\n-    array([array([0]), array([1, 2])], dtype=object)\n-    \"\"\"\n-    out = np.empty(len(sequence), dtype=object)\n-    out[:] = sequence\n-    return out\n-\n-\n-def indices_to_mask(indices, mask_length):\n-    \"\"\"Convert list of indices to boolean mask.\n-\n-    Parameters\n-    ----------\n-    indices : list-like\n-        List of integers treated as indices.\n-    mask_length : int\n-        Length of boolean mask to be generated.\n-        This parameter must be greater than max(indices).\n-\n-    Returns\n-    -------\n-    mask : 1d boolean nd-array\n-        Boolean array that is True where indices are present, else False.\n-\n-    Examples\n-    --------\n-    >>> from sklearn.utils import indices_to_mask\n-    >>> indices = [1, 2 , 3, 4]\n-    >>> indices_to_mask(indices, 5)\n-    array([False,  True,  True,  True,  True])\n-    \"\"\"\n-    if mask_length <= np.max(indices):\n-        raise ValueError(\"mask_length must be greater than max(indices)\")\n-\n-    mask = np.zeros(mask_length, dtype=bool)\n-    mask[indices] = True\n-\n-    return mask\n-\n-\n-def _message_with_time(source, message, time):\n-    \"\"\"Create one line message for logging purposes.\n-\n-    Parameters\n-    ----------\n-    source : str\n-        String indicating the source or the reference of the message.\n-\n-    message : str\n-        Short message.\n-\n-    time : int\n-        Time in seconds.\n-    \"\"\"\n-    start_message = \"[%s] \" % source\n-\n-    # adapted from joblib.logger.short_format_time without the Windows -.1s\n-    # adjustment\n-    if time > 60:\n-        time_str = \"%4.1fmin\" % (time / 60)\n-    else:\n-        time_str = \" %5.1fs\" % time\n-    end_message = \" %s, total=%s\" % (message, time_str)\n-    dots_len = 70 - len(start_message) - len(end_message)\n-    return \"%s%s%s\" % (start_message, dots_len * \".\", end_message)\n-\n-\n-@contextmanager\n-def _print_elapsed_time(source, message=None):\n-    \"\"\"Log elapsed time to stdout when the context is exited.\n-\n-    Parameters\n-    ----------\n-    source : str\n-        String indicating the source or the reference of the message.\n-\n-    message : str, default=None\n-        Short message. If None, nothing will be printed.\n-\n-    Returns\n-    -------\n-    context_manager\n-        Prints elapsed time upon exit if verbose.\n-    \"\"\"\n-    if message is None:\n-        yield\n-    else:\n-        start = timeit.default_timer()\n-        yield\n-        print(_message_with_time(source, message, timeit.default_timer() - start))\n-\n-\n-def get_chunk_n_rows(row_bytes, *, max_n_rows=None, working_memory=None):\n-    \"\"\"Calculate how many rows can be processed within `working_memory`.\n-\n-    Parameters\n-    ----------\n-    row_bytes : int\n-        The expected number of bytes of memory that will be consumed\n-        during the processing of each row.\n-    max_n_rows : int, default=None\n-        The maximum return value.\n-    working_memory : int or float, default=None\n-        The number of rows to fit inside this number of MiB will be\n-        returned. When None (default), the value of\n-        ``sklearn.get_config()['working_memory']`` is used.\n-\n-    Returns\n-    -------\n-    int\n-        The number of rows which can be processed within `working_memory`.\n-\n-    Warns\n-    -----\n-    Issues a UserWarning if `row_bytes exceeds `working_memory` MiB.\n-    \"\"\"\n-\n-    if working_memory is None:\n-        working_memory = get_config()[\"working_memory\"]\n-\n-    chunk_n_rows = int(working_memory * (2**20) // row_bytes)\n-    if max_n_rows is not None:\n-        chunk_n_rows = min(chunk_n_rows, max_n_rows)\n-    if chunk_n_rows < 1:\n-        warnings.warn(\n-            \"Could not adhere to working_memory config. \"\n-            \"Currently %.0fMiB, %.0fMiB required.\"\n-            % (working_memory, np.ceil(row_bytes * 2**-20))\n-        )\n-        chunk_n_rows = 1\n-    return chunk_n_rows\n-\n-\n-def _is_pandas_na(x):\n-    \"\"\"Test if x is pandas.NA.\n-\n-    We intentionally do not use this function to return `True` for `pd.NA` in\n-    `is_scalar_nan`, because estimators that support `pd.NA` are the exception\n-    rather than the rule at the moment. When `pd.NA` is more universally\n-    supported, we may reconsider this decision.\n-\n-    Parameters\n-    ----------\n-    x : any type\n-\n-    Returns\n-    -------\n-    boolean\n-    \"\"\"\n-    with suppress(ImportError):\n-        from pandas import NA\n-\n-        return x is NA\n-\n-    return False\n-\n-\n-def is_scalar_nan(x):\n-    \"\"\"Test if x is NaN.\n-\n-    This function is meant to overcome the issue that np.isnan does not allow\n-    non-numerical types as input, and that np.nan is not float('nan').\n-\n-    Parameters\n-    ----------\n-    x : any type\n-        Any scalar value.\n-\n-    Returns\n-    -------\n-    bool\n-        Returns true if x is NaN, and false otherwise.\n-\n-    Examples\n-    --------\n-    >>> import numpy as np\n-    >>> from sklearn.utils import is_scalar_nan\n-    >>> is_scalar_nan(np.nan)\n-    True\n-    >>> is_scalar_nan(float(\"nan\"))\n-    True\n-    >>> is_scalar_nan(None)\n-    False\n-    >>> is_scalar_nan(\"\")\n-    False\n-    >>> is_scalar_nan([np.nan])\n-    False\n-    \"\"\"\n-    return isinstance(x, numbers.Real) and math.isnan(x)\n-\n-\n-def _approximate_mode(class_counts, n_draws, rng):\n-    \"\"\"Computes approximate mode of multivariate hypergeometric.\n-\n-    This is an approximation to the mode of the multivariate\n-    hypergeometric given by class_counts and n_draws.\n-    It shouldn't be off by more than one.\n-\n-    It is the mostly likely outcome of drawing n_draws many\n-    samples from the population given by class_counts.\n-\n-    Parameters\n-    ----------\n-    class_counts : ndarray of int\n-        Population per class.\n-    n_draws : int\n-        Number of draws (samples to draw) from the overall population.\n-    rng : random state\n-        Used to break ties.\n-\n-    Returns\n-    -------\n-    sampled_classes : ndarray of int\n-        Number of samples drawn from each class.\n-        np.sum(sampled_classes) == n_draws\n-\n-    Examples\n-    --------\n-    >>> import numpy as np\n-    >>> from sklearn.utils import _approximate_mode\n-    >>> _approximate_mode(class_counts=np.array([4, 2]), n_draws=3, rng=0)\n-    array([2, 1])\n-    >>> _approximate_mode(class_counts=np.array([5, 2]), n_draws=4, rng=0)\n-    array([3, 1])\n-    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),\n-    ...                   n_draws=2, rng=0)\n-    array([0, 1, 1, 0])\n-    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),\n-    ...                   n_draws=2, rng=42)\n-    array([1, 1, 0, 0])\n-    \"\"\"\n-    rng = check_random_state(rng)\n-    # this computes a bad approximation to the mode of the\n-    # multivariate hypergeometric given by class_counts and n_draws\n-    continuous = class_counts / class_counts.sum() * n_draws\n-    # floored means we don't overshoot n_samples, but probably undershoot\n-    floored = np.floor(continuous)\n-    # we add samples according to how much \"left over\" probability\n-    # they had, until we arrive at n_samples\n-    need_to_add = int(n_draws - floored.sum())\n-    if need_to_add > 0:\n-        remainder = continuous - floored\n-        values = np.sort(np.unique(remainder))[::-1]\n-        # add according to remainder, but break ties\n-        # randomly to avoid biases\n-        for value in values:\n-            (inds,) = np.where(remainder == value)\n-            # if we need_to_add less than what's in inds\n-            # we draw randomly from them.\n-            # if we need to add more, we add them all and\n-            # go to the next value\n-            add_now = min(len(inds), need_to_add)\n-            inds = rng.choice(inds, size=add_now, replace=False)\n-            floored[inds] += 1\n-            need_to_add -= add_now\n-            if need_to_add == 0:\n-                break\n-    return floored.astype(int)\n-\n-\n-def check_matplotlib_support(caller_name):\n-    \"\"\"Raise ImportError with detailed error message if mpl is not installed.\n-\n-    Plot utilities like any of the Display's plotting functions should lazily import\n-    matplotlib and call this helper before any computation.\n-\n-    Parameters\n-    ----------\n-    caller_name : str\n-        The name of the caller that requires matplotlib.\n-    \"\"\"\n-    try:\n-        import matplotlib  # noqa\n-    except ImportError as e:\n-        raise ImportError(\n-            \"{} requires matplotlib. You can install matplotlib with \"\n-            \"`pip install matplotlib`\".format(caller_name)\n-        ) from e\n-\n-\n-def check_pandas_support(caller_name):\n-    \"\"\"Raise ImportError with detailed error message if pandas is not installed.\n-\n-    Plot utilities like :func:`fetch_openml` should lazily import\n-    pandas and call this helper before any computation.\n-\n-    Parameters\n-    ----------\n-    caller_name : str\n-        The name of the caller that requires pandas.\n-\n-    Returns\n-    -------\n-    pandas\n-        The pandas package.\n+    Check if an object is array-like.\n     \"\"\"\n-    try:\n-        import pandas  # noqa\n+    return hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes))\n \n-        return pandas\n-    except ImportError as e:\n-        raise ImportError(\"{} requires pandas.\".format(caller_name)) from e\ndiff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\nindex 5eaef2fde..e1dfc4aa4 100644\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -1,521 +1,35 @@\n-# Author: Arnaud Joly, Joel Nothman, Hamzeh Alsalhi\n-#\n-# License: BSD 3 clause\n-\"\"\"\n-Multi-class / multi-label utility function\n-==========================================\n-\n-\"\"\"\n-from collections.abc import Sequence\n-from itertools import chain\n-import warnings\n-\n-from scipy.sparse import issparse\n-from scipy.sparse import dok_matrix\n-from scipy.sparse import lil_matrix\n-\n import numpy as np\n-\n-from .validation import check_array, _assert_all_finite\n-from ..utils._array_api import get_namespace\n-\n-\n-def _unique_multiclass(y):\n-    xp, is_array_api = get_namespace(y)\n-    if hasattr(y, \"__array__\") or is_array_api:\n-        return xp.unique_values(xp.asarray(y))\n-    else:\n-        return set(y)\n-\n-\n-def _unique_indicator(y):\n-    return np.arange(\n-        check_array(y, input_name=\"y\", accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1]\n-    )\n-\n-\n-_FN_UNIQUE_LABELS = {\n-    \"binary\": _unique_multiclass,\n-    \"multiclass\": _unique_multiclass,\n-    \"multilabel-indicator\": _unique_indicator,\n-}\n-\n+from sklearn.utils import _is_arraylike\n+import pandas as pd\n \n def unique_labels(*ys):\n-    \"\"\"Extract an ordered array of unique labels.\n-\n-    We don't allow:\n-        - mix of multilabel and multiclass (single label) targets\n-        - mix of label indicator matrix and anything else,\n-          because there are no explicit labels)\n-        - mix of label indicator matrices of different sizes\n-        - mix of string and integer labels\n-\n-    At the moment, we also don't allow \"multiclass-multioutput\" input type.\n-\n-    Parameters\n-    ----------\n-    *ys : array-likes\n-        Label values.\n-\n-    Returns\n-    -------\n-    out : ndarray of shape (n_unique_labels,)\n-        An ordered array of unique labels.\n-\n-    Examples\n-    --------\n-    >>> from sklearn.utils.multiclass import unique_labels\n-    >>> unique_labels([3, 5, 5, 5, 7, 7])\n-    array([3, 5, 7])\n-    >>> unique_labels([1, 2, 3, 4], [2, 2, 3, 4])\n-    array([1, 2, 3, 4])\n-    >>> unique_labels([1, 2, 10], [5, 11])\n-    array([ 1,  2,  5, 10, 11])\n-    \"\"\"\n-    xp, is_array_api = get_namespace(*ys)\n-    if not ys:\n-        raise ValueError(\"No argument has been passed.\")\n-    # Check that we don't mix label format\n-\n-    ys_types = set(type_of_target(x) for x in ys)\n-    if ys_types == {\"binary\", \"multiclass\"}:\n-        ys_types = {\"multiclass\"}\n-\n-    if len(ys_types) > 1:\n-        raise ValueError(\"Mix type of y not allowed, got types %s\" % ys_types)\n-\n-    label_type = ys_types.pop()\n-\n-    # Check consistency for the indicator format\n-    if (\n-        label_type == \"multilabel-indicator\"\n-        and len(\n-            set(\n-                check_array(y, accept_sparse=[\"csr\", \"csc\", \"coo\"]).shape[1] for y in ys\n-            )\n-        )\n-        > 1\n-    ):\n-        raise ValueError(\n-            \"Multi-label binary indicator input with different numbers of labels\"\n-        )\n-\n-    # Get the unique set of labels\n-    _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)\n-    if not _unique_labels:\n-        raise ValueError(\"Unknown label type: %s\" % repr(ys))\n-\n-    if is_array_api:\n-        # array_api does not allow for mixed dtypes\n-        unique_ys = xp.concat([_unique_labels(y) for y in ys])\n-        return xp.unique_values(unique_ys)\n-\n-    ys_labels = set(chain.from_iterable((i for i in _unique_labels(y)) for y in ys))\n-    # Check that we don't mix string type with number type\n-    if len(set(isinstance(label, str) for label in ys_labels)) > 1:\n-        raise ValueError(\"Mix of label input types (string and number)\")\n-\n-    return xp.asarray(sorted(ys_labels))\n-\n-\n-def _is_integral_float(y):\n-    return y.dtype.kind == \"f\" and np.all(y.astype(int) == y)\n-\n-\n-def is_multilabel(y):\n-    \"\"\"Check if ``y`` is in a multilabel format.\n-\n-    Parameters\n-    ----------\n-    y : ndarray of shape (n_samples,)\n-        Target values.\n-\n-    Returns\n-    -------\n-    out : bool\n-        Return ``True``, if ``y`` is in a multilabel format, else ```False``.\n-\n-    Examples\n-    --------\n-    >>> import numpy as np\n-    >>> from sklearn.utils.multiclass import is_multilabel\n-    >>> is_multilabel([0, 1, 0, 1])\n-    False\n-    >>> is_multilabel([[1], [0, 2], []])\n-    False\n-    >>> is_multilabel(np.array([[1, 0], [0, 0]]))\n-    True\n-    >>> is_multilabel(np.array([[1], [0], [0]]))\n-    False\n-    >>> is_multilabel(np.array([[1, 0, 0]]))\n-    True\n     \"\"\"\n-    xp, is_array_api = get_namespace(y)\n-    if hasattr(y, \"__array__\") or isinstance(y, Sequence) or is_array_api:\n-        # DeprecationWarning will be replaced by ValueError, see NEP 34\n-        # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n-        with warnings.catch_warnings():\n-            warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n-            try:\n-                y = xp.asarray(y)\n-            except (np.VisibleDeprecationWarning, ValueError):\n-                # dtype=object should be provided explicitly for ragged arrays,\n-                # see NEP 34\n-                y = xp.asarray(y, dtype=object)\n-\n-    if not (hasattr(y, \"shape\") and y.ndim == 2 and y.shape[1] > 1):\n-        return False\n-\n-    if issparse(y):\n-        if isinstance(y, (dok_matrix, lil_matrix)):\n-            y = y.tocsr()\n-        labels = xp.unique_values(y.data)\n-        return (\n-            len(y.data) == 0\n-            or (labels.size == 1 or (labels.size == 2) and (0 in labels))\n-            and (y.dtype.kind in \"biu\" or _is_integral_float(labels))  # bool, int, uint\n-        )\n-    else:\n-        labels = xp.unique_values(y)\n-\n-        return len(labels) < 3 and (\n-            y.dtype.kind in \"biu\" or _is_integral_float(labels)  # bool, int, uint\n-        )\n-\n-\n-def check_classification_targets(y):\n-    \"\"\"Ensure that target y is of a non-regression type.\n-\n-    Only the following target types (as defined in type_of_target) are allowed:\n-        'binary', 'multiclass', 'multiclass-multioutput',\n-        'multilabel-indicator', 'multilabel-sequences'\n+    Find the unique classes or labels in one or more label arrays.\n+    This function is useful for determining the range of possible values\n+    that a classifier may output, given some training data.\n \n     Parameters\n     ----------\n-    y : array-like\n-        Target values.\n+    *ys : array-like\n+        One or more label arrays. Each array must have shape (n_samples,) and\n+        contain integer or string labels.\n     \"\"\"\n-    y_type = type_of_target(y, input_name=\"y\")\n-    if y_type not in [\n-        \"binary\",\n-        \"multiclass\",\n-        \"multiclass-multioutput\",\n-        \"multilabel-indicator\",\n-        \"multilabel-sequences\",\n-    ]:\n-        raise ValueError(\"Unknown label type: %r\" % y_type)\n-\n-\n-def type_of_target(y, input_name=\"\"):\n-    \"\"\"Determine the type of data indicated by the target.\n-\n-    Note that this type is the most specific type that can be inferred.\n-    For example:\n-\n-        * ``binary`` is more specific but compatible with ``multiclass``.\n-        * ``multiclass`` of integers is more specific but compatible with\n-          ``continuous``.\n-        * ``multilabel-indicator`` is more specific but compatible with\n-          ``multiclass-multioutput``.\n-\n-    Parameters\n-    ----------\n-    y : {array-like, sparse matrix}\n-        Target values. If a sparse matrix, `y` is expected to be a\n-        CSR/CSC matrix.\n-\n-    input_name : str, default=\"\"\n-        The data name used to construct the error message.\n-\n-        .. versionadded:: 1.1.0\n-\n-    Returns\n-    -------\n-    target_type : str\n-        One of:\n-\n-        * 'continuous': `y` is an array-like of floats that are not all\n-          integers, and is 1d or a column vector.\n-        * 'continuous-multioutput': `y` is a 2d array of floats that are\n-          not all integers, and both dimensions are of size > 1.\n-        * 'binary': `y` contains <= 2 discrete values and is 1d or a column\n-          vector.\n-        * 'multiclass': `y` contains more than two discrete values, is not a\n-          sequence of sequences, and is 1d or a column vector.\n-        * 'multiclass-multioutput': `y` is a 2d array that contains more\n-          than two discrete values, is not a sequence of sequences, and both\n-          dimensions are of size > 1.\n-        * 'multilabel-indicator': `y` is a label indicator matrix, an array\n-          of two dimensions with at least two columns, and at most 2 unique\n-          values.\n-        * 'unknown': `y` is array-like but none of the above, such as a 3d\n-          array, sequence of sequences, or an array of non-sequence objects.\n-\n-    Examples\n-    --------\n-    >>> from sklearn.utils.multiclass import type_of_target\n-    >>> import numpy as np\n-    >>> type_of_target([0.1, 0.6])\n-    'continuous'\n-    >>> type_of_target([1, -1, -1, 1])\n-    'binary'\n-    >>> type_of_target(['a', 'b', 'a'])\n-    'binary'\n-    >>> type_of_target([1.0, 2.0])\n-    'binary'\n-    >>> type_of_target([1, 0, 2])\n-    'multiclass'\n-    >>> type_of_target([1.0, 0.0, 3.0])\n-    'multiclass'\n-    >>> type_of_target(['a', 'b', 'c'])\n-    'multiclass'\n-    >>> type_of_target(np.array([[1, 2], [3, 1]]))\n-    'multiclass-multioutput'\n-    >>> type_of_target([[1, 2]])\n-    'multilabel-indicator'\n-    >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n-    'continuous-multioutput'\n-    >>> type_of_target(np.array([[0, 1], [1, 1]]))\n-    'multilabel-indicator'\n-    \"\"\"\n-    xp, is_array_api = get_namespace(y)\n-    valid = (\n-        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n-        and not isinstance(y, str)\n-        or is_array_api\n-    )\n-\n-    if not valid:\n-        raise ValueError(\n-            \"Expected array-like (array or non-string sequence), got %r\" % y\n-        )\n-\n-    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n-    if sparse_pandas:\n-        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n-\n-    if is_multilabel(y):\n-        return \"multilabel-indicator\"\n-\n-    # DeprecationWarning will be replaced by ValueError, see NEP 34\n-    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n-    # We therefore catch both deprecation (NumPy < 1.24) warning and\n-    # value error (NumPy >= 1.24).\n-    with warnings.catch_warnings():\n-        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n-        if not issparse(y):\n-            try:\n-                y = xp.asarray(y)\n-            except (np.VisibleDeprecationWarning, ValueError):\n-                # dtype=object should be provided explicitly for ragged arrays,\n-                # see NEP 34\n-                y = xp.asarray(y, dtype=object)\n-\n-    # The old sequence of sequences format\n-    try:\n-        if (\n-            not hasattr(y[0], \"__array__\")\n-            and isinstance(y[0], Sequence)\n-            and not isinstance(y[0], str)\n-        ):\n-            raise ValueError(\n-                \"You appear to be using a legacy multi-label data\"\n-                \" representation. Sequence of sequences are no\"\n-                \" longer supported; use a binary array or sparse\"\n-                \" matrix instead - the MultiLabelBinarizer\"\n-                \" transformer can convert to this format.\"\n-            )\n-    except IndexError:\n-        pass\n-\n-    # Invalid inputs\n-    if y.ndim not in (1, 2):\n-        # Number of dimension greater than 2: [[[1, 2]]]\n-        return \"unknown\"\n-    if not min(y.shape):\n-        # Empty ndarray: []/[[]]\n-        if y.ndim == 1:\n-            # 1-D empty array: []\n-            return \"binary\"  # []\n-        # 2-D empty array: [[]]\n-        return \"unknown\"\n-    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n-        # [obj_1] and not [\"label_1\"]\n-        return \"unknown\"\n-\n-    # Check if multioutput\n-    if y.ndim == 2 and y.shape[1] > 1:\n-        suffix = \"-multioutput\"  # [[1, 2], [1, 2]]\n-    else:\n-        suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n-\n-    # Check float and contains non-integer float values\n-    if y.dtype.kind == \"f\":\n-        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n-        data = y.data if issparse(y) else y\n-        if xp.any(data != data.astype(int)):\n-            _assert_all_finite(data, input_name=input_name)\n-            return \"continuous\" + suffix\n-\n-    # Check multiclass\n-    first_row = y[0] if not issparse(y) else y.getrow(0).data\n-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n-        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n-        return \"multiclass\" + suffix\n-    else:\n-        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n-\n-\n-def _check_partial_fit_first_call(clf, classes=None):\n-    \"\"\"Private helper function for factorizing common classes param logic.\n-\n-    Estimators that implement the ``partial_fit`` API need to be provided with\n-    the list of possible classes at the first call to partial_fit.\n-\n-    Subsequent calls to partial_fit should check that ``classes`` is still\n-    consistent with a previous value of ``clf.classes_`` when provided.\n-\n-    This function returns True if it detects that this was the first call to\n-    ``partial_fit`` on ``clf``. In that case the ``classes_`` attribute is also\n-    set on ``clf``.\n-\n-    \"\"\"\n-    if getattr(clf, \"classes_\", None) is None and classes is None:\n-        raise ValueError(\"classes must be passed on the first call to partial_fit.\")\n-\n-    elif classes is not None:\n-        if getattr(clf, \"classes_\", None) is not None:\n-            if not np.array_equal(clf.classes_, unique_labels(classes)):\n-                raise ValueError(\n-                    \"`classes=%r` is not the same as on last call \"\n-                    \"to partial_fit, was: %r\" % (classes, clf.classes_)\n-                )\n-\n-        else:\n-            # This is the first call to partial_fit\n-            clf.classes_ = unique_labels(classes)\n-            return True\n-\n-    # classes is None and clf.classes_ has already previously been set:\n-    # nothing to do\n-    return False\n-\n-\n-def class_distribution(y, sample_weight=None):\n-    \"\"\"Compute class priors from multioutput-multiclass target data.\n-\n-    Parameters\n-    ----------\n-    y : {array-like, sparse matrix} of size (n_samples, n_outputs)\n-        The labels for each example.\n-\n-    sample_weight : array-like of shape (n_samples,), default=None\n-        Sample weights.\n-\n-    Returns\n-    -------\n-    classes : list of size n_outputs of ndarray of size (n_classes,)\n-        List of classes for each column.\n-\n-    n_classes : list of int of size n_outputs\n-        Number of classes in each column.\n-\n-    class_prior : list of size n_outputs of ndarray of size (n_classes,)\n-        Class distribution of each column.\n-    \"\"\"\n-    classes = []\n-    n_classes = []\n-    class_prior = []\n-\n-    n_samples, n_outputs = y.shape\n-    if sample_weight is not None:\n-        sample_weight = np.asarray(sample_weight)\n-\n-    if issparse(y):\n-        y = y.tocsc()\n-        y_nnz = np.diff(y.indptr)\n-\n-        for k in range(n_outputs):\n-            col_nonzero = y.indices[y.indptr[k] : y.indptr[k + 1]]\n-            # separate sample weights for zero and non-zero elements\n-            if sample_weight is not None:\n-                nz_samp_weight = sample_weight[col_nonzero]\n-                zeros_samp_weight_sum = np.sum(sample_weight) - np.sum(nz_samp_weight)\n-            else:\n-                nz_samp_weight = None\n-                zeros_samp_weight_sum = y.shape[0] - y_nnz[k]\n-\n-            classes_k, y_k = np.unique(\n-                y.data[y.indptr[k] : y.indptr[k + 1]], return_inverse=True\n-            )\n-            class_prior_k = np.bincount(y_k, weights=nz_samp_weight)\n-\n-            # An explicit zero was found, combine its weight with the weight\n-            # of the implicit zeros\n-            if 0 in classes_k:\n-                class_prior_k[classes_k == 0] += zeros_samp_weight_sum\n-\n-            # If an there is an implicit zero and it is not in classes and\n-            # class_prior, make an entry for it\n-            if 0 not in classes_k and y_nnz[k] < y.shape[0]:\n-                classes_k = np.insert(classes_k, 0, 0)\n-                class_prior_k = np.insert(class_prior_k, 0, zeros_samp_weight_sum)\n-\n-            classes.append(classes_k)\n-            n_classes.append(classes_k.shape[0])\n-            class_prior.append(class_prior_k / class_prior_k.sum())\n-    else:\n-        for k in range(n_outputs):\n-            classes_k, y_k = np.unique(y[:, k], return_inverse=True)\n-            classes.append(classes_k)\n-            n_classes.append(classes_k.shape[0])\n-            class_prior_k = np.bincount(y_k, weights=sample_weight)\n-            class_prior.append(class_prior_k / class_prior_k.sum())\n-\n-    return (classes, n_classes, class_prior)\n-\n-\n-def _ovr_decision_function(predictions, confidences, n_classes):\n-    \"\"\"Compute a continuous, tie-breaking OvR decision function from OvO.\n-\n-    It is important to include a continuous value, not only votes,\n-    to make computing AUC or calibration meaningful.\n-\n-    Parameters\n-    ----------\n-    predictions : array-like of shape (n_samples, n_classifiers)\n-        Predicted classes for each binary classifier.\n-\n-    confidences : array-like of shape (n_samples, n_classifiers)\n-        Decision functions or predicted probabilities for positive class\n-        for each binary classifier.\n-\n-    n_classes : int\n-        Number of classes. n_classifiers must be\n-        ``n_classes * (n_classes - 1 ) / 2``.\n-    \"\"\"\n-    n_samples = predictions.shape[0]\n-    votes = np.zeros((n_samples, n_classes))\n-    sum_of_confidences = np.zeros((n_samples, n_classes))\n-\n-    k = 0\n-    for i in range(n_classes):\n-        for j in range(i + 1, n_classes):\n-            sum_of_confidences[:, i] -= confidences[:, k]\n-            sum_of_confidences[:, j] += confidences[:, k]\n-            votes[predictions[:, k] == 0, i] += 1\n-            votes[predictions[:, k] == 1, j] += 1\n-            k += 1\n-\n-    # Monotonically transform the sum_of_confidences to (-1/3, 1/3)\n-    # and add it with votes. The monotonic transformation  is\n-    # f: x -> x / (3 * (|x| + 1)), it uses 1/3 instead of 1/2\n-    # to ensure that we won't reach the limits and change vote order.\n-    # The motivation is to use confidence levels as a way to break ties in\n-    # the votes without switching any decision made based on a difference\n-    # of 1 vote.\n-    transformed_confidences = sum_of_confidences / (\n-        3 * (np.abs(sum_of_confidences) + 1)\n-    )\n-    return votes + transformed_confidences\n+    # Convert input to numpy arrays if they are not already\n+    ys = [np.asarray(y) for y in ys]\n+\n+    # Check for nullable pandas dtypes and convert them appropriately\n+    for i, y in enumerate(ys):\n+        if isinstance(y.dtype, pd.CategoricalDtype):\n+            ys[i] = y.astype(str)\n+        elif str(y.dtype) == \"Int64\":\n+            ys[i] = y.astype(np.int64)\n+        elif str(y.dtype) == \"Float64\":\n+            ys[i] = y.astype(np.float64)\n+        elif str(y.dtype) == \"boolean\":\n+            ys[i] = y.astype(bool)\n+\n+    # Flatten the list of arrays and find unique labels\n+    all_labels = np.concatenate(ys)\n+    unique_labels = np.unique(all_labels)\n+\n+    return unique_labels\ndiff --git a/tests/test_multiclass.py b/tests/test_multiclass.py\nnew file mode 100644\nindex 000000000..c0867a65e\n--- /dev/null\n+++ b/tests/test_multiclass.py\n@@ -0,0 +1,13 @@\n+import pytest\n+from sklearn.utils import unique_labels\n+import pandas as pd\n+\n+def test_unique_labels_with_nullable_dtypes():\n+    for dtype in [\"Int64\", \"Float64\", \"boolean\"]:\n+        y_true = pd.Series([1, 0, 0, 1, 0, 1, 1, 0, 1], dtype=dtype)\n+        y_predicted = pd.Series([0, 0, 1, 1, 0, 1, 1, 1, 1], dtype=\"int64\")\n+\n+        result = unique_labels(y_true, y_predicted)\n+\n+        # Add assertions to verify the correctness of the function\n+        assert set(result) == {0, 1}\n",
            "model": "ollama_chat/mistral-small",
            "temperature": 0,
            "cost": 0.0,
            "added_files": [
                "sklearn/utils/__init__.py",
                "sklearn/utils/multiclass.py",
                "tests/test_multiclass.py"
            ],
            "gold_files": [
                "sklearn/utils/multiclass.py"
            ],
            "edited_files": [
                "sklearn/utils/__init__.py",
                "sklearn/utils/multiclass.py",
                "tests/test_multiclass.py"
            ],
            "edit_outcome": false,
            "lint_outcome": true,
            "test_outcome": false,
            "try": 1
        }
    ]
}